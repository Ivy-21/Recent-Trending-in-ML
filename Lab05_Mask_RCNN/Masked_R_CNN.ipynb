{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Masked-R-CNN.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BU5w0emGydD"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import errno\n",
        "import os\n",
        "import time\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "\n",
        "\n",
        "class SmoothedValue:\n",
        "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
        "    window or the global series average.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, window_size=20, fmt=None):\n",
        "        if fmt is None:\n",
        "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
        "        self.deque = deque(maxlen=window_size)\n",
        "        self.total = 0.0\n",
        "        self.count = 0\n",
        "        self.fmt = fmt\n",
        "\n",
        "    def update(self, value, n=1):\n",
        "        self.deque.append(value)\n",
        "        self.count += n\n",
        "        self.total += value * n\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        \"\"\"\n",
        "        Warning: does not synchronize the deque!\n",
        "        \"\"\"\n",
        "        if not is_dist_avail_and_initialized():\n",
        "            return\n",
        "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device=\"cuda\")\n",
        "        dist.barrier()\n",
        "        dist.all_reduce(t)\n",
        "        t = t.tolist()\n",
        "        self.count = int(t[0])\n",
        "        self.total = t[1]\n",
        "\n",
        "    @property\n",
        "    def median(self):\n",
        "        d = torch.tensor(list(self.deque))\n",
        "        return d.median().item()\n",
        "\n",
        "    @property\n",
        "    def avg(self):\n",
        "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
        "        return d.mean().item()\n",
        "\n",
        "    @property\n",
        "    def global_avg(self):\n",
        "        return self.total / self.count\n",
        "\n",
        "    @property\n",
        "    def max(self):\n",
        "        return max(self.deque)\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        return self.deque[-1]\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.fmt.format(\n",
        "            median=self.median, avg=self.avg, global_avg=self.global_avg, max=self.max, value=self.value\n",
        "        )\n",
        "\n",
        "\n",
        "def all_gather(data):\n",
        "    \"\"\"\n",
        "    Run all_gather on arbitrary picklable data (not necessarily tensors)\n",
        "    Args:\n",
        "        data: any picklable object\n",
        "    Returns:\n",
        "        list[data]: list of data gathered from each rank\n",
        "    \"\"\"\n",
        "    world_size = get_world_size()\n",
        "    if world_size == 1:\n",
        "        return [data]\n",
        "    data_list = [None] * world_size\n",
        "    dist.all_gather_object(data_list, data)\n",
        "    return data_list\n",
        "\n",
        "\n",
        "def reduce_dict(input_dict, average=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        input_dict (dict): all the values will be reduced\n",
        "        average (bool): whether to do average or sum\n",
        "    Reduce the values in the dictionary from all processes so that all processes\n",
        "    have the averaged results. Returns a dict with the same fields as\n",
        "    input_dict, after reduction.\n",
        "    \"\"\"\n",
        "    world_size = get_world_size()\n",
        "    if world_size < 2:\n",
        "        return input_dict\n",
        "    with torch.inference_mode():\n",
        "        names = []\n",
        "        values = []\n",
        "        # sort the keys so that they are consistent across processes\n",
        "        for k in sorted(input_dict.keys()):\n",
        "            names.append(k)\n",
        "            values.append(input_dict[k])\n",
        "        values = torch.stack(values, dim=0)\n",
        "        dist.all_reduce(values)\n",
        "        if average:\n",
        "            values /= world_size\n",
        "        reduced_dict = {k: v for k, v in zip(names, values)}\n",
        "    return reduced_dict\n",
        "\n",
        "\n",
        "class MetricLogger:\n",
        "    def __init__(self, delimiter=\"\\t\"):\n",
        "        self.meters = defaultdict(SmoothedValue)\n",
        "        self.delimiter = delimiter\n",
        "\n",
        "    def update(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                v = v.item()\n",
        "            assert isinstance(v, (float, int))\n",
        "            self.meters[k].update(v)\n",
        "\n",
        "    def __getattr__(self, attr):\n",
        "        if attr in self.meters:\n",
        "            return self.meters[attr]\n",
        "        if attr in self.__dict__:\n",
        "            return self.__dict__[attr]\n",
        "        raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{attr}'\")\n",
        "\n",
        "    def __str__(self):\n",
        "        loss_str = []\n",
        "        for name, meter in self.meters.items():\n",
        "            loss_str.append(f\"{name}: {str(meter)}\")\n",
        "        return self.delimiter.join(loss_str)\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        for meter in self.meters.values():\n",
        "            meter.synchronize_between_processes()\n",
        "\n",
        "    def add_meter(self, name, meter):\n",
        "        self.meters[name] = meter\n",
        "\n",
        "    def log_every(self, iterable, print_freq, header=None):\n",
        "        i = 0\n",
        "        if not header:\n",
        "            header = \"\"\n",
        "        start_time = time.time()\n",
        "        end = time.time()\n",
        "        iter_time = SmoothedValue(fmt=\"{avg:.4f}\")\n",
        "        data_time = SmoothedValue(fmt=\"{avg:.4f}\")\n",
        "        space_fmt = \":\" + str(len(str(len(iterable)))) + \"d\"\n",
        "        if torch.cuda.is_available():\n",
        "            log_msg = self.delimiter.join(\n",
        "                [\n",
        "                    header,\n",
        "                    \"[{0\" + space_fmt + \"}/{1}]\",\n",
        "                    \"eta: {eta}\",\n",
        "                    \"{meters}\",\n",
        "                    \"time: {time}\",\n",
        "                    \"data: {data}\",\n",
        "                    \"max mem: {memory:.0f}\",\n",
        "                ]\n",
        "            )\n",
        "        else:\n",
        "            log_msg = self.delimiter.join(\n",
        "                [header, \"[{0\" + space_fmt + \"}/{1}]\", \"eta: {eta}\", \"{meters}\", \"time: {time}\", \"data: {data}\"]\n",
        "            )\n",
        "        MB = 1024.0 * 1024.0\n",
        "        for obj in iterable:\n",
        "            data_time.update(time.time() - end)\n",
        "            yield obj\n",
        "            iter_time.update(time.time() - end)\n",
        "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
        "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
        "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
        "                if torch.cuda.is_available():\n",
        "                    print(\n",
        "                        log_msg.format(\n",
        "                            i,\n",
        "                            len(iterable),\n",
        "                            eta=eta_string,\n",
        "                            meters=str(self),\n",
        "                            time=str(iter_time),\n",
        "                            data=str(data_time),\n",
        "                            memory=torch.cuda.max_memory_allocated() / MB,\n",
        "                        )\n",
        "                    )\n",
        "                else:\n",
        "                    print(\n",
        "                        log_msg.format(\n",
        "                            i, len(iterable), eta=eta_string, meters=str(self), time=str(iter_time), data=str(data_time)\n",
        "                        )\n",
        "                    )\n",
        "            i += 1\n",
        "            end = time.time()\n",
        "        total_time = time.time() - start_time\n",
        "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "        print(f\"{header} Total time: {total_time_str} ({total_time / len(iterable):.4f} s / it)\")\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "\n",
        "def mkdir(path):\n",
        "    try:\n",
        "        os.makedirs(path)\n",
        "    except OSError as e:\n",
        "        if e.errno != errno.EEXIST:\n",
        "            raise\n",
        "\n",
        "\n",
        "def setup_for_distributed(is_master):\n",
        "    \"\"\"\n",
        "    This function disables printing when not in master process\n",
        "    \"\"\"\n",
        "    import builtins as __builtin__\n",
        "\n",
        "    builtin_print = __builtin__.print\n",
        "\n",
        "    def print(*args, **kwargs):\n",
        "        force = kwargs.pop(\"force\", False)\n",
        "        if is_master or force:\n",
        "            builtin_print(*args, **kwargs)\n",
        "\n",
        "    __builtin__.print = print\n",
        "\n",
        "\n",
        "def is_dist_avail_and_initialized():\n",
        "    if not dist.is_available():\n",
        "        return False\n",
        "    if not dist.is_initialized():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def get_world_size():\n",
        "    if not is_dist_avail_and_initialized():\n",
        "        return 1\n",
        "    return dist.get_world_size()\n",
        "\n",
        "\n",
        "def get_rank():\n",
        "    if not is_dist_avail_and_initialized():\n",
        "        return 0\n",
        "    return dist.get_rank()\n",
        "\n",
        "\n",
        "def is_main_process():\n",
        "    return get_rank() == 0\n",
        "\n",
        "\n",
        "def save_on_master(*args, **kwargs):\n",
        "    if is_main_process():\n",
        "        torch.save(*args, **kwargs)\n",
        "\n",
        "\n",
        "def init_distributed_mode(args):\n",
        "    if \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ:\n",
        "        args.rank = int(os.environ[\"RANK\"])\n",
        "        args.world_size = int(os.environ[\"WORLD_SIZE\"])\n",
        "        args.gpu = int(os.environ[\"LOCAL_RANK\"])\n",
        "    elif \"SLURM_PROCID\" in os.environ:\n",
        "        args.rank = int(os.environ[\"SLURM_PROCID\"])\n",
        "        args.gpu = args.rank % torch.cuda.device_count()\n",
        "    else:\n",
        "        print(\"Not using distributed mode\")\n",
        "        args.distributed = False\n",
        "        return\n",
        "\n",
        "    args.distributed = True\n",
        "\n",
        "    torch.cuda.set_device(args.gpu)\n",
        "    args.dist_backend = \"nccl\"\n",
        "    print(f\"| distributed init (rank {args.rank}): {args.dist_url}\", flush=True)\n",
        "    torch.distributed.init_process_group(\n",
        "        backend=args.dist_backend, init_method=args.dist_url, world_size=args.world_size, rank=args.rank\n",
        "    )\n",
        "    torch.distributed.barrier()\n",
        "    setup_for_distributed(args.rank == 0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from pycocotools import mask as coco_mask\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "\n",
        "class FilterAndRemapCocoCategories:\n",
        "    def __init__(self, categories, remap=True):\n",
        "        self.categories = categories\n",
        "        self.remap = remap\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        anno = target[\"annotations\"]\n",
        "        anno = [obj for obj in anno if obj[\"category_id\"] in self.categories]\n",
        "        if not self.remap:\n",
        "            target[\"annotations\"] = anno\n",
        "            return image, target\n",
        "        anno = copy.deepcopy(anno)\n",
        "        for obj in anno:\n",
        "            obj[\"category_id\"] = self.categories.index(obj[\"category_id\"])\n",
        "        target[\"annotations\"] = anno\n",
        "        return image, target\n",
        "\n",
        "\n",
        "def convert_coco_poly_to_mask(segmentations, height, width):\n",
        "    masks = []\n",
        "    for polygons in segmentations:\n",
        "        rles = coco_mask.frPyObjects(polygons, height, width)\n",
        "        mask = coco_mask.decode(rles)\n",
        "        if len(mask.shape) < 3:\n",
        "            mask = mask[..., None]\n",
        "        mask = torch.as_tensor(mask, dtype=torch.uint8)\n",
        "        mask = mask.any(dim=2)\n",
        "        masks.append(mask)\n",
        "    if masks:\n",
        "        masks = torch.stack(masks, dim=0)\n",
        "    else:\n",
        "        masks = torch.zeros((0, height, width), dtype=torch.uint8)\n",
        "    return masks\n",
        "\n",
        "\n",
        "class ConvertCocoPolysToMask:\n",
        "    def __call__(self, image, target):\n",
        "        w, h = image.size\n",
        "\n",
        "        image_id = target[\"image_id\"]\n",
        "        image_id = torch.tensor([image_id])\n",
        "\n",
        "        anno = target[\"annotations\"]\n",
        "\n",
        "        anno = [obj for obj in anno if obj[\"iscrowd\"] == 0]\n",
        "\n",
        "        boxes = [obj[\"bbox\"] for obj in anno]\n",
        "        # guard against no boxes via resizing\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
        "        boxes[:, 2:] += boxes[:, :2]\n",
        "        boxes[:, 0::2].clamp_(min=0, max=w)\n",
        "        boxes[:, 1::2].clamp_(min=0, max=h)\n",
        "\n",
        "        classes = [obj[\"category_id\"] for obj in anno]\n",
        "        classes = torch.tensor(classes, dtype=torch.int64)\n",
        "\n",
        "        segmentations = [obj[\"segmentation\"] for obj in anno]\n",
        "        masks = convert_coco_poly_to_mask(segmentations, h, w)\n",
        "\n",
        "        keypoints = None\n",
        "        if anno and \"keypoints\" in anno[0]:\n",
        "            keypoints = [obj[\"keypoints\"] for obj in anno]\n",
        "            keypoints = torch.as_tensor(keypoints, dtype=torch.float32)\n",
        "            num_keypoints = keypoints.shape[0]\n",
        "            if num_keypoints:\n",
        "                keypoints = keypoints.view(num_keypoints, -1, 3)\n",
        "\n",
        "        keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n",
        "        boxes = boxes[keep]\n",
        "        classes = classes[keep]\n",
        "        masks = masks[keep]\n",
        "        if keypoints is not None:\n",
        "            keypoints = keypoints[keep]\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = classes\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        if keypoints is not None:\n",
        "            target[\"keypoints\"] = keypoints\n",
        "\n",
        "        # for conversion to coco api\n",
        "        area = torch.tensor([obj[\"area\"] for obj in anno])\n",
        "        iscrowd = torch.tensor([obj[\"iscrowd\"] for obj in anno])\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        return image, target\n",
        "\n",
        "\n",
        "def _coco_remove_images_without_annotations(dataset, cat_list=None):\n",
        "    def _has_only_empty_bbox(anno):\n",
        "        return all(any(o <= 1 for o in obj[\"bbox\"][2:]) for obj in anno)\n",
        "\n",
        "    def _count_visible_keypoints(anno):\n",
        "        return sum(sum(1 for v in ann[\"keypoints\"][2::3] if v > 0) for ann in anno)\n",
        "\n",
        "    min_keypoints_per_image = 10\n",
        "\n",
        "    def _has_valid_annotation(anno):\n",
        "        # if it's empty, there is no annotation\n",
        "        if len(anno) == 0:\n",
        "            return False\n",
        "        # if all boxes have close to zero area, there is no annotation\n",
        "        if _has_only_empty_bbox(anno):\n",
        "            return False\n",
        "        # keypoints task have a slight different critera for considering\n",
        "        # if an annotation is valid\n",
        "        if \"keypoints\" not in anno[0]:\n",
        "            return True\n",
        "        # for keypoint detection tasks, only consider valid images those\n",
        "        # containing at least min_keypoints_per_image\n",
        "        if _count_visible_keypoints(anno) >= min_keypoints_per_image:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    assert isinstance(dataset, torchvision.datasets.CocoDetection)\n",
        "    ids = []\n",
        "    for ds_idx, img_id in enumerate(dataset.ids):\n",
        "        ann_ids = dataset.coco.getAnnIds(imgIds=img_id, iscrowd=None)\n",
        "        anno = dataset.coco.loadAnns(ann_ids)\n",
        "        if cat_list:\n",
        "            anno = [obj for obj in anno if obj[\"category_id\"] in cat_list]\n",
        "        if _has_valid_annotation(anno):\n",
        "            ids.append(ds_idx)\n",
        "\n",
        "    dataset = torch.utils.data.Subset(dataset, ids)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def convert_to_coco_api(ds):\n",
        "    coco_ds = COCO()\n",
        "    # annotation IDs need to start at 1, not 0, see torchvision issue #1530\n",
        "    ann_id = 1\n",
        "    dataset = {\"images\": [], \"categories\": [], \"annotations\": []}\n",
        "    categories = set()\n",
        "    for img_idx in range(len(ds)):\n",
        "        # find better way to get target\n",
        "        # targets = ds.get_annotations(img_idx)\n",
        "        img, targets = ds[img_idx]\n",
        "        image_id = targets[\"image_id\"].item()\n",
        "        img_dict = {}\n",
        "        img_dict[\"id\"] = image_id\n",
        "        img_dict[\"height\"] = img.shape[-2]\n",
        "        img_dict[\"width\"] = img.shape[-1]\n",
        "        dataset[\"images\"].append(img_dict)\n",
        "        bboxes = targets[\"boxes\"].clone()\n",
        "        bboxes[:, 2:] -= bboxes[:, :2]\n",
        "        bboxes = bboxes.tolist()\n",
        "        labels = targets[\"labels\"].tolist()\n",
        "        areas = targets[\"area\"].tolist()\n",
        "        iscrowd = targets[\"iscrowd\"].tolist()\n",
        "        if \"masks\" in targets:\n",
        "            masks = targets[\"masks\"]\n",
        "            # make masks Fortran contiguous for coco_mask\n",
        "            masks = masks.permute(0, 2, 1).contiguous().permute(0, 2, 1)\n",
        "        if \"keypoints\" in targets:\n",
        "            keypoints = targets[\"keypoints\"]\n",
        "            keypoints = keypoints.reshape(keypoints.shape[0], -1).tolist()\n",
        "        num_objs = len(bboxes)\n",
        "        for i in range(num_objs):\n",
        "            ann = {}\n",
        "            ann[\"image_id\"] = image_id\n",
        "            ann[\"bbox\"] = bboxes[i]\n",
        "            ann[\"category_id\"] = labels[i]\n",
        "            categories.add(labels[i])\n",
        "            ann[\"area\"] = areas[i]\n",
        "            ann[\"iscrowd\"] = iscrowd[i]\n",
        "            ann[\"id\"] = ann_id\n",
        "            if \"masks\" in targets:\n",
        "                ann[\"segmentation\"] = coco_mask.encode(masks[i].numpy())\n",
        "            if \"keypoints\" in targets:\n",
        "                ann[\"keypoints\"] = keypoints[i]\n",
        "                ann[\"num_keypoints\"] = sum(k != 0 for k in keypoints[i][2::3])\n",
        "            dataset[\"annotations\"].append(ann)\n",
        "            ann_id += 1\n",
        "    dataset[\"categories\"] = [{\"id\": i} for i in sorted(categories)]\n",
        "    coco_ds.dataset = dataset\n",
        "    coco_ds.createIndex()\n",
        "    return coco_ds\n",
        "\n",
        "\n",
        "def get_coco_api_from_dataset(dataset):\n",
        "    for _ in range(10):\n",
        "        if isinstance(dataset, torchvision.datasets.CocoDetection):\n",
        "            break\n",
        "        if isinstance(dataset, torch.utils.data.Subset):\n",
        "            dataset = dataset.dataset\n",
        "    if isinstance(dataset, torchvision.datasets.CocoDetection):\n",
        "        return dataset.coco\n",
        "    return convert_to_coco_api(dataset)\n",
        "\n",
        "\n",
        "class CocoDetection(torchvision.datasets.CocoDetection):\n",
        "    def __init__(self, img_folder, ann_file, transforms):\n",
        "        super().__init__(img_folder, ann_file)\n",
        "        self._transforms = transforms\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, target = super().__getitem__(idx)\n",
        "        image_id = self.ids[idx]\n",
        "        target = dict(image_id=image_id, annotations=target)\n",
        "        if self._transforms is not None:\n",
        "            img, target = self._transforms(img, target)\n",
        "        return img, target\n",
        "\n",
        "\n",
        "def get_coco(root, image_set, transforms, mode=\"instances\"):\n",
        "    anno_file_template = \"{}_{}2017.json\"\n",
        "    PATHS = {\n",
        "        \"train\": (\"train2017\", os.path.join(\"annotations\", anno_file_template.format(mode, \"train\"))),\n",
        "        \"val\": (\"val2017\", os.path.join(\"annotations\", anno_file_template.format(mode, \"val\"))),\n",
        "        # \"train\": (\"val2017\", os.path.join(\"annotations\", anno_file_template.format(mode, \"val\")))\n",
        "    }\n",
        "\n",
        "    t = [ConvertCocoPolysToMask()]\n",
        "\n",
        "    if transforms is not None:\n",
        "        t.append(transforms)\n",
        "    transforms = T.Compose(t)\n",
        "\n",
        "    img_folder, ann_file = PATHS[image_set]\n",
        "    img_folder = os.path.join(root, img_folder)\n",
        "    ann_file = os.path.join(root, ann_file)\n",
        "\n",
        "    dataset = CocoDetection(img_folder, ann_file, transforms=transforms)\n",
        "\n",
        "    if image_set == \"train\":\n",
        "        dataset = _coco_remove_images_without_annotations(dataset)\n",
        "\n",
        "    # dataset = torch.utils.data.Subset(dataset, [i for i in range(500)])\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def get_coco_kp(root, image_set, transforms):\n",
        "    return get_coco(root, image_set, transforms, mode=\"person_keypoints\")\n",
        "\n",
        "\n",
        "def get_cityscapes(root, ann_file, transforms):\n",
        "    t = [ConvertCocoPolysToMask()]\n",
        "\n",
        "    if transforms is not None:\n",
        "        t.append(transforms)\n",
        "    transforms = T.Compose(t)\n",
        "    dataset = CocoDetection(root, ann_file, transforms=transforms)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "hMgUCMSLxTC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple, Dict, Optional\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn, Tensor\n",
        "from torchvision.transforms import functional as F\n",
        "from torchvision.transforms import transforms as T, InterpolationMode\n",
        "\n",
        "\n",
        "def _flip_coco_person_keypoints(kps, width):\n",
        "    flip_inds = [0, 2, 1, 4, 3, 6, 5, 8, 7, 10, 9, 12, 11, 14, 13, 16, 15]\n",
        "    flipped_data = kps[:, flip_inds]\n",
        "    flipped_data[..., 0] = width - flipped_data[..., 0]\n",
        "    # Maintain COCO convention that if visibility == 0, then x, y = 0\n",
        "    inds = flipped_data[..., 2] == 0\n",
        "    flipped_data[inds] = 0\n",
        "    return flipped_data\n",
        "\n",
        "\n",
        "class Compose:\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target\n",
        "\n",
        "\n",
        "class RandomHorizontalFlip(T.RandomHorizontalFlip):\n",
        "    def forward(\n",
        "        self, image: Tensor, target: Optional[Dict[str, Tensor]] = None\n",
        "    ) -> Tuple[Tensor, Optional[Dict[str, Tensor]]]:\n",
        "        if torch.rand(1) < self.p:\n",
        "            image = F.hflip(image)\n",
        "            if target is not None:\n",
        "                _, _, width = F.get_dimensions(image)\n",
        "                target[\"boxes\"][:, [0, 2]] = width - target[\"boxes\"][:, [2, 0]]\n",
        "                if \"masks\" in target:\n",
        "                    target[\"masks\"] = target[\"masks\"].flip(-1)\n",
        "                if \"keypoints\" in target:\n",
        "                    keypoints = target[\"keypoints\"]\n",
        "                    keypoints = _flip_coco_person_keypoints(keypoints, width)\n",
        "                    target[\"keypoints\"] = keypoints\n",
        "        return image, target\n",
        "\n",
        "\n",
        "class ToTensor(nn.Module):\n",
        "    def forward(\n",
        "        self, image: Tensor, target: Optional[Dict[str, Tensor]] = None\n",
        "    ) -> Tuple[Tensor, Optional[Dict[str, Tensor]]]:\n",
        "        image = F.pil_to_tensor(image)\n",
        "        image = F.convert_image_dtype(image)\n",
        "        return image, target\n",
        "\n",
        "\n",
        "class PILToTensor(nn.Module):\n",
        "    def forward(\n",
        "        self, image: Tensor, target: Optional[Dict[str, Tensor]] = None\n",
        "    ) -> Tuple[Tensor, Optional[Dict[str, Tensor]]]:\n",
        "        image = F.pil_to_tensor(image)\n",
        "        return image, target\n",
        "\n",
        "\n",
        "class ConvertImageDtype(nn.Module):\n",
        "    def __init__(self, dtype: torch.dtype) -> None:\n",
        "        super().__init__()\n",
        "        self.dtype = dtype\n",
        "\n",
        "    def forward(\n",
        "        self, image: Tensor, target: Optional[Dict[str, Tensor]] = None\n",
        "    ) -> Tuple[Tensor, Optional[Dict[str, Tensor]]]:\n",
        "        image = F.convert_image_dtype(image, self.dtype)\n",
        "        return image, target\n",
        "\n",
        "\n",
        "class RandomIoUCrop(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        min_scale: float = 0.3,\n",
        "        max_scale: float = 1.0,\n",
        "        min_aspect_ratio: float = 0.5,\n",
        "        max_aspect_ratio: float = 2.0,\n",
        "        sampler_options: Optional[List[float]] = None,\n",
        "        trials: int = 40,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # Configuration similar to https://github.com/weiliu89/caffe/blob/ssd/examples/ssd/ssd_coco.py#L89-L174\n",
        "        self.min_scale = min_scale\n",
        "        self.max_scale = max_scale\n",
        "        self.min_aspect_ratio = min_aspect_ratio\n",
        "        self.max_aspect_ratio = max_aspect_ratio\n",
        "        if sampler_options is None:\n",
        "            sampler_options = [0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0]\n",
        "        self.options = sampler_options\n",
        "        self.trials = trials\n",
        "\n",
        "    def forward(\n",
        "        self, image: Tensor, target: Optional[Dict[str, Tensor]] = None\n",
        "    ) -> Tuple[Tensor, Optional[Dict[str, Tensor]]]:\n",
        "        if target is None:\n",
        "            raise ValueError(\"The targets can't be None for this transform.\")\n",
        "\n",
        "        if isinstance(image, torch.Tensor):\n",
        "            if image.ndimension() not in {2, 3}:\n",
        "                raise ValueError(f\"image should be 2/3 dimensional. Got {image.ndimension()} dimensions.\")\n",
        "            elif image.ndimension() == 2:\n",
        "                image = image.unsqueeze(0)\n",
        "\n",
        "        _, orig_h, orig_w = F.get_dimensions(image)\n",
        "\n",
        "        while True:\n",
        "            # sample an option\n",
        "            idx = int(torch.randint(low=0, high=len(self.options), size=(1,)))\n",
        "            min_jaccard_overlap = self.options[idx]\n",
        "            if min_jaccard_overlap >= 1.0:  # a value larger than 1 encodes the leave as-is option\n",
        "                return image, target\n",
        "\n",
        "            for _ in range(self.trials):\n",
        "                # check the aspect ratio limitations\n",
        "                r = self.min_scale + (self.max_scale - self.min_scale) * torch.rand(2)\n",
        "                new_w = int(orig_w * r[0])\n",
        "                new_h = int(orig_h * r[1])\n",
        "                aspect_ratio = new_w / new_h\n",
        "                if not (self.min_aspect_ratio <= aspect_ratio <= self.max_aspect_ratio):\n",
        "                    continue\n",
        "\n",
        "                # check for 0 area crops\n",
        "                r = torch.rand(2)\n",
        "                left = int((orig_w - new_w) * r[0])\n",
        "                top = int((orig_h - new_h) * r[1])\n",
        "                right = left + new_w\n",
        "                bottom = top + new_h\n",
        "                if left == right or top == bottom:\n",
        "                    continue\n",
        "\n",
        "                # check for any valid boxes with centers within the crop area\n",
        "                cx = 0.5 * (target[\"boxes\"][:, 0] + target[\"boxes\"][:, 2])\n",
        "                cy = 0.5 * (target[\"boxes\"][:, 1] + target[\"boxes\"][:, 3])\n",
        "                is_within_crop_area = (left < cx) & (cx < right) & (top < cy) & (cy < bottom)\n",
        "                if not is_within_crop_area.any():\n",
        "                    continue\n",
        "\n",
        "                # check at least 1 box with jaccard limitations\n",
        "                boxes = target[\"boxes\"][is_within_crop_area]\n",
        "                ious = torchvision.ops.boxes.box_iou(\n",
        "                    boxes, torch.tensor([[left, top, right, bottom]], dtype=boxes.dtype, device=boxes.device)\n",
        "                )\n",
        "                if ious.max() < min_jaccard_overlap:\n",
        "                    continue\n",
        "\n",
        "                # keep only valid boxes and perform cropping\n",
        "                target[\"boxes\"] = boxes\n",
        "                target[\"labels\"] = target[\"labels\"][is_within_crop_area]\n",
        "                target[\"boxes\"][:, 0::2] -= left\n",
        "                target[\"boxes\"][:, 1::2] -= top\n",
        "                target[\"boxes\"][:, 0::2].clamp_(min=0, max=new_w)\n",
        "                target[\"boxes\"][:, 1::2].clamp_(min=0, max=new_h)\n",
        "                image = F.crop(image, top, left, new_h, new_w)\n",
        "\n",
        "                return image, target\n",
        "\n",
        "\n",
        "class RandomZoomOut(nn.Module):\n",
        "    def __init__(\n",
        "        self, fill: Optional[List[float]] = None, side_range: Tuple[float, float] = (1.0, 4.0), p: float = 0.5\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if fill is None:\n",
        "            fill = [0.0, 0.0, 0.0]\n",
        "        self.fill = fill\n",
        "        self.side_range = side_range\n",
        "        if side_range[0] < 1.0 or side_range[0] > side_range[1]:\n",
        "            raise ValueError(f\"Invalid canvas side range provided {side_range}.\")\n",
        "        self.p = p\n",
        "\n",
        "    @torch.jit.unused\n",
        "    def _get_fill_value(self, is_pil):\n",
        "        # type: (bool) -> int\n",
        "        # We fake the type to make it work on JIT\n",
        "        return tuple(int(x) for x in self.fill) if is_pil else 0\n",
        "\n",
        "    def forward(\n",
        "        self, image: Tensor, target: Optional[Dict[str, Tensor]] = None\n",
        "    ) -> Tuple[Tensor, Optional[Dict[str, Tensor]]]:\n",
        "        if isinstance(image, torch.Tensor):\n",
        "            if image.ndimension() not in {2, 3}:\n",
        "                raise ValueError(f\"image should be 2/3 dimensional. Got {image.ndimension()} dimensions.\")\n",
        "            elif image.ndimension() == 2:\n",
        "                image = image.unsqueeze(0)\n",
        "\n",
        "        if torch.rand(1) >= self.p:\n",
        "            return image, target\n",
        "\n",
        "        _, orig_h, orig_w = F.get_dimensions(image)\n",
        "\n",
        "        r = self.side_range[0] + torch.rand(1) * (self.side_range[1] - self.side_range[0])\n",
        "        canvas_width = int(orig_w * r)\n",
        "        canvas_height = int(orig_h * r)\n",
        "\n",
        "        r = torch.rand(2)\n",
        "        left = int((canvas_width - orig_w) * r[0])\n",
        "        top = int((canvas_height - orig_h) * r[1])\n",
        "        right = canvas_width - (left + orig_w)\n",
        "        bottom = canvas_height - (top + orig_h)\n",
        "\n",
        "        if torch.jit.is_scripting():\n",
        "            fill = 0\n",
        "        else:\n",
        "            fill = self._get_fill_value(F._is_pil_image(image))\n",
        "\n",
        "        image = F.pad(image, [left, top, right, bottom], fill=fill)\n",
        "        if isinstance(image, torch.Tensor):\n",
        "            # PyTorch's pad supports only integers on fill. So we need to overwrite the colour\n",
        "            v = torch.tensor(self.fill, device=image.device, dtype=image.dtype).view(-1, 1, 1)\n",
        "            image[..., :top, :] = image[..., :, :left] = image[..., (top + orig_h) :, :] = image[\n",
        "                ..., :, (left + orig_w) :\n",
        "            ] = v\n",
        "\n",
        "        if target is not None:\n",
        "            target[\"boxes\"][:, 0::2] += left\n",
        "            target[\"boxes\"][:, 1::2] += top\n",
        "\n",
        "        return image, target\n",
        "\n",
        "\n",
        "class RandomPhotometricDistort(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        contrast: Tuple[float] = (0.5, 1.5),\n",
        "        saturation: Tuple[float] = (0.5, 1.5),\n",
        "        hue: Tuple[float] = (-0.05, 0.05),\n",
        "        brightness: Tuple[float] = (0.875, 1.125),\n",
        "        p: float = 0.5,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self._brightness = T.ColorJitter(brightness=brightness)\n",
        "        self._contrast = T.ColorJitter(contrast=contrast)\n",
        "        self._hue = T.ColorJitter(hue=hue)\n",
        "        self._saturation = T.ColorJitter(saturation=saturation)\n",
        "        self.p = p\n",
        "\n",
        "    def forward(\n",
        "        self, image: Tensor, target: Optional[Dict[str, Tensor]] = None\n",
        "    ) -> Tuple[Tensor, Optional[Dict[str, Tensor]]]:\n",
        "        if isinstance(image, torch.Tensor):\n",
        "            if image.ndimension() not in {2, 3}:\n",
        "                raise ValueError(f\"image should be 2/3 dimensional. Got {image.ndimension()} dimensions.\")\n",
        "            elif image.ndimension() == 2:\n",
        "                image = image.unsqueeze(0)\n",
        "\n",
        "        r = torch.rand(7)\n",
        "\n",
        "        if r[0] < self.p:\n",
        "            image = self._brightness(image)\n",
        "\n",
        "        contrast_before = r[1] < 0.5\n",
        "        if contrast_before:\n",
        "            if r[2] < self.p:\n",
        "                image = self._contrast(image)\n",
        "\n",
        "        if r[3] < self.p:\n",
        "            image = self._saturation(image)\n",
        "\n",
        "        if r[4] < self.p:\n",
        "            image = self._hue(image)\n",
        "\n",
        "        if not contrast_before:\n",
        "            if r[5] < self.p:\n",
        "                image = self._contrast(image)\n",
        "\n",
        "        if r[6] < self.p:\n",
        "            channels, _, _ = F.get_dimensions(image)\n",
        "            permutation = torch.randperm(channels)\n",
        "\n",
        "            is_pil = F._is_pil_image(image)\n",
        "            if is_pil:\n",
        "                image = F.pil_to_tensor(image)\n",
        "                image = F.convert_image_dtype(image)\n",
        "            image = image[..., permutation, :, :]\n",
        "            if is_pil:\n",
        "                image = F.to_pil_image(image)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "\n",
        "class ScaleJitter(nn.Module):\n",
        "    \"\"\"Randomly resizes the image and its bounding boxes  within the specified scale range.\n",
        "    The class implements the Scale Jitter augmentation as described in the paper\n",
        "    `\"Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation\" <https://arxiv.org/abs/2012.07177>`_.\n",
        "    Args:\n",
        "        target_size (tuple of ints): The target size for the transform provided in (height, weight) format.\n",
        "        scale_range (tuple of ints): scaling factor interval, e.g (a, b), then scale is randomly sampled from the\n",
        "            range a <= scale <= b.\n",
        "        interpolation (InterpolationMode): Desired interpolation enum defined by\n",
        "            :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.BILINEAR``.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        target_size: Tuple[int, int],\n",
        "        scale_range: Tuple[float, float] = (0.1, 2.0),\n",
        "        interpolation: InterpolationMode = InterpolationMode.BILINEAR,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.target_size = target_size\n",
        "        self.scale_range = scale_range\n",
        "        self.interpolation = interpolation\n",
        "\n",
        "    def forward(\n",
        "        self, image: Tensor, target: Optional[Dict[str, Tensor]] = None\n",
        "    ) -> Tuple[Tensor, Optional[Dict[str, Tensor]]]:\n",
        "        if isinstance(image, torch.Tensor):\n",
        "            if image.ndimension() not in {2, 3}:\n",
        "                raise ValueError(f\"image should be 2/3 dimensional. Got {image.ndimension()} dimensions.\")\n",
        "            elif image.ndimension() == 2:\n",
        "                image = image.unsqueeze(0)\n",
        "\n",
        "        _, orig_height, orig_width = F.get_dimensions(image)\n",
        "\n",
        "        r = self.scale_range[0] + torch.rand(1) * (self.scale_range[1] - self.scale_range[0])\n",
        "        new_width = int(self.target_size[1] * r)\n",
        "        new_height = int(self.target_size[0] * r)\n",
        "\n",
        "        image = F.resize(image, [new_height, new_width], interpolation=self.interpolation)\n",
        "\n",
        "        if target is not None:\n",
        "            target[\"boxes\"][:, 0::2] *= new_width / orig_width\n",
        "            target[\"boxes\"][:, 1::2] *= new_height / orig_height\n",
        "            if \"masks\" in target:\n",
        "                target[\"masks\"] = F.resize(\n",
        "                    target[\"masks\"], [new_height, new_width], interpolation=InterpolationMode.NEAREST\n",
        "                )\n",
        "\n",
        "        return image, target"
      ],
      "metadata": {
        "id": "u_aHeS9HxlnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transforms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzpOXJ0-xxdv",
        "outputId": "e84baa52-8948-428c-e77d-27661ab2b65c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transforms\n",
            "  Downloading transforms-0.1.tar.gz (29 kB)\n",
            "Building wheels for collected packages: transforms\n",
            "  Building wheel for transforms (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transforms: filename=transforms-0.1-py3-none-any.whl size=39351 sha256=e584ded85a54015dfb57c6071a2d597520b7eb58e244e42d91eb0ab382ebb708\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/8d/26/be8b7e278f01d5093d18fc99c8158bd8528403242c852a558c\n",
            "Successfully built transforms\n",
            "Installing collected packages: transforms\n",
            "Successfully installed transforms-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "from torchvision.datasets import CocoDetection\n",
        "import transforms\n",
        "import su\n",
        "\n",
        "# import utils\n",
        "# from coco_utils import get_coco\n",
        "# import transforms\n",
        "\n",
        "# Load a model pre-trained on COCO and put it in inference mode\n",
        "\n",
        "print('Loading pretrained model...')\n",
        "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Load the COCO 2017 train and val sets. We use the CocoDetection class definition\n",
        "# from ./coco_utils.py, not the original torchvision.CocoDetection class. Also, we\n",
        "# use transforms from ./transforms, not torchvision.transforms, because they need\n",
        "# to transform the bboxes and masks along with the image.\n",
        "\n",
        "coco_path = \"/home/jovyan/work/COCO\"\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "print('Loading COCO train, val datasets...')\n",
        "coco_train_dataset = get_coco(coco_path, 'train', transform)\n",
        "coco_val_dataset = get_coco(coco_path, 'val', transform)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "val_dataloader = torch.utils.data.DataLoader(coco_val_dataset, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "FlVG3prQIlGT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "outputId": "1246c30a-4536-442f-cf0b-e91b2f1805c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-f10a63f8e5a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_rcnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMaskRCNNPredictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCocoDetection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# import utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transforms/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msafe_html\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msafe_html\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbodyfinder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# modules = [\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     'st',             # zopish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     'rest',           # docutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transforms/safe_html.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msgmllib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSGMLParser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSGMLParseError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcgi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mescape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msafeToInt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sgmllib'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images, targets = next(iter(val_dataloader))\n",
        "images = [ img.cuda() for img in images ]\n",
        "predictions = model(images)\n",
        "\n",
        "print('Prediction keys:', list(dict(predictions[0])))\n",
        "print('Boxes shape:', predictions[0]['boxes'].shape)\n",
        "print('Labels shape:', predictions[0]['labels'].shape)\n",
        "print('Scores shape:', predictions[0]['scores'].shape)\n",
        "print('Masks shape:', predictions[0]['masks'].shape)"
      ],
      "metadata": {
        "id": "att-TlVFInV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "\n",
        "# Array of labels for COCO dataset (91 elements)\n",
        "\n",
        "coco_names = [\n",
        "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
        "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
        "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
        "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
        "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
        "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
        "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
        "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
        "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
        "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
        "]\n",
        "\n",
        "# Random colors to use for labeling objects\n",
        "\n",
        "COLORS = np.random.uniform(0, 255, size=(len(coco_names), 3)).astype(np.uint8)\n",
        "\n",
        "# Overlay masks, bounding boxes, and labels on input numpy image\n",
        "\n",
        "def draw_segmentation_map(image, masks, boxes, labels):\n",
        "    alpha = 1\n",
        "    beta = 0.5 # transparency for the segmentation map\n",
        "    gamma = 0 # scalar added to each sum\n",
        "    # convert from RGB to OpenCV BGR format\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "    for i in range(len(masks)):\n",
        "        mask = masks[i,:,:]\n",
        "        red_map = np.zeros_like(mask).astype(np.uint8)\n",
        "        green_map = np.zeros_like(mask).astype(np.uint8)\n",
        "        blue_map = np.zeros_like(mask).astype(np.uint8)\n",
        "        # apply a randon color mask to each object\n",
        "        color = COLORS[random.randrange(0, len(COLORS))]\n",
        "        red_map[mask > 0.5] = color[0]\n",
        "        green_map[mask > 0.5] = color[1]\n",
        "        blue_map[mask > 0.5] = color[2]\n",
        "        # combine all the masks into a single image\n",
        "        segmentation_map = np.stack([red_map, green_map, blue_map], axis=2)\n",
        "        # apply colored mask to the image\n",
        "        image = cv2.addWeighted(image, alpha, segmentation_map, beta, gamma)\n",
        "        # draw the bounding box around each object\n",
        "        p1 = (int(boxes[i][0]), int(boxes[i][1]))\n",
        "        p2 = (int(boxes[i][2]), int(boxes[i][3]))\n",
        "        color = (int(color[0]), int(color[1]), int(color[2]))\n",
        "        cv2.rectangle(image, p1, p2, color, 2)\n",
        "        # put the label text above the objects\n",
        "        p = (int(boxes[i][0]), int(boxes[i][1]-10))\n",
        "        cv2.putText(image, labels[i], p, cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2, cv2.LINE_AA)\n",
        "    \n",
        "    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Overlay masks, bounding boxes, and labels of objects with scores greater than\n",
        "# threshold on one of the images in the input tensor using the predictions output by Mask R-CNN.\n",
        "\n",
        "def prediction_to_mask_image(images, predictions, img_index, threshold):\n",
        "    scores = predictions[img_index]['scores']\n",
        "    boxes_to_use = scores >= threshold\n",
        "    img = (images[img_index].cpu().permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
        "    masks = predictions[img_index]['masks'][boxes_to_use, :, :].cpu().detach().squeeze(1).numpy()\n",
        "    boxes = predictions[img_index]['boxes'][boxes_to_use, :].cpu().detach().numpy()\n",
        "    labels = predictions[img_index]['labels'][boxes_to_use].cpu().numpy()\n",
        "    labels = [ coco_names[l] for l in labels ]\n",
        "\n",
        "    return draw_segmentation_map(img, masks, boxes, labels)"
      ],
      "metadata": {
        "id": "HJs0SpFZIppx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "masked_img = prediction_to_mask_image(images, predictions, 0, 0.5)\n",
        "plt.figure(1, figsize=(12, 9), dpi=100)\n",
        "plt.imshow(masked_img)\n",
        "plt.title('Validation image result')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5AGudqKQK4Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NBMGiBvPqrRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FineTune"
      ],
      "metadata": {
        "id": "DRmlVOsTrADj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n"
      ],
      "metadata": {
        "id": "dQog013orCFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2021 Old_ques\n"
      ],
      "metadata": {
        "id": "pmsSoaPDyvzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "from PIL import Image\n",
        "\n",
        "num_classes = 8\n",
        "\n",
        "model = torchvision.models.detection.maskrcnn_resnet50_fpn()\n",
        "\n",
        "# Modify model for the given number of classes\n",
        "\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "num_classes = 8\n",
        "\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "hidden_layer = 256\n",
        "model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
        "                                                       hidden_layer,\n",
        "                                                       num_classes)\n",
        "\n",
        "print('Loading weights from file')\n",
        "model.load_state_dict(torch.load('mask-rcnn-coco-pretrain-cityscapes-finetune-10-epochs.pth', map_location=torch.device('cpu')))\n",
        "model.eval()\n",
        "\n",
        "im = Image.open('20201112_072342.jpg').rotate(180)\n",
        "\n",
        "import torchvision.transforms as T\n",
        "\n",
        "transform = T.ToTensor()\n",
        "image_batch = transform(im).unsqueeze(0)\n",
        "image_batch.shape\n",
        "\n",
        "predictions = model(image_batch)\n",
        "\n",
        "print('Prediction keys:', list(dict(predictions[0])))\n",
        "print('Boxes shape:', predictions[0]['boxes'].shape)\n",
        "print('Labels shape:', predictions[0]['labels'].shape)\n",
        "print('Scores shape:', predictions[0]['scores'].shape)\n",
        "print('Masks shape:', predictions[0]['masks'].shape)\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "\n",
        "class_names = [\n",
        "    'bicycle', 'car', 'motorcycle', 'person', 'rider', 'truck', 'bus', 'train'\n",
        "]\n",
        "\n",
        "# Random colors to use for labeling objects\n",
        "\n",
        "COLORS = np.random.uniform(0, 255, size=(len(class_names), 3)).astype(np.uint8)\n",
        "\n",
        "# Overlay masks, bounding boxes, and labels on input numpy image\n",
        "\n",
        "def draw_segmentation_map(image, masks, boxes, labels):\n",
        "    alpha = 1\n",
        "    beta = 0.5 # transparency for the segmentation map\n",
        "    gamma = 0 # scalar added to each sum\n",
        "    # convert from RGB to OpenCV BGR format\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "    for i in range(len(masks)):\n",
        "        mask = masks[i,:,:]\n",
        "        red_map = np.zeros_like(mask).astype(np.uint8)\n",
        "        green_map = np.zeros_like(mask).astype(np.uint8)\n",
        "        blue_map = np.zeros_like(mask).astype(np.uint8)\n",
        "        # apply a randon color mask to each object\n",
        "        color = COLORS[random.randrange(0, len(COLORS))]\n",
        "        red_map[mask > 0.5] = color[0]\n",
        "        green_map[mask > 0.5] = color[1]\n",
        "        blue_map[mask > 0.5] = color[2]\n",
        "        # combine all the masks into a single image\n",
        "        segmentation_map = np.stack([red_map, green_map, blue_map], axis=2)\n",
        "        # apply colored mask to the image\n",
        "        image = cv2.addWeighted(image, alpha, segmentation_map, beta, gamma)\n",
        "        # draw the bounding box around each object\n",
        "        p1 = (int(boxes[i][0]), int(boxes[i][1]))\n",
        "        p2 = (int(boxes[i][2]), int(boxes[i][3]))\n",
        "        color = (int(color[0]), int(color[1]), int(color[2]))\n",
        "        cv2.rectangle(image, p1, p2, color, 2)\n",
        "        # put the label text above the objects\n",
        "        p = (int(boxes[i][0]), int(boxes[i][1]-10))\n",
        "        cv2.putText(image, labels[i], p, cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2, cv2.LINE_AA)\n",
        "    \n",
        "    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Overlay masks, bounding boxes, and labels of objects with scores greater than\n",
        "# threshold on one of the images in the input tensor using the predictions output by Mask R-CNN.\n",
        "\n",
        "def prediction_to_mask_image(images, predictions, img_index, threshold):\n",
        "    scores = predictions[img_index]['scores']\n",
        "    boxes_to_use = scores >= threshold\n",
        "    img = (images[img_index].cpu().permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
        "    masks = predictions[img_index]['masks'][boxes_to_use, :, :].cpu().detach().squeeze(1).numpy()\n",
        "    boxes = predictions[img_index]['boxes'][boxes_to_use, :].cpu().detach().numpy()\n",
        "    labels = predictions[img_index]['labels'][boxes_to_use].cpu().numpy()\n",
        "    labels = [ class_names[l] for l in labels ]\n",
        "\n",
        "    return draw_segmentation_map(img, masks, boxes, labels)\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "masked_img = prediction_to_mask_image(image_batch, predictions, 0, 0.5)\n",
        "plt.figure(1, figsize=(12, 9), dpi=100)\n",
        "plt.imshow(masked_img)\n",
        "plt.title('Validation image result')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0V4LGSdYyyWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.cs.ait.ac.th/~mdailey/20201112_072342.jpg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUrlsLrure0C",
        "outputId": "3dabd9b0-c25f-48a4-9e3a-2f76505b29e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-03 12:08:22--  https://www.cs.ait.ac.th/~mdailey/20201112_072342.jpg\n",
            "Resolving www.cs.ait.ac.th (www.cs.ait.ac.th)... 192.41.170.42\n",
            "Connecting to www.cs.ait.ac.th (www.cs.ait.ac.th)|192.41.170.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3320786 (3.2M) [image/jpeg]\n",
            "Saving to: 20201112_072342.jpg\n",
            "\n",
            "20201112_072342.jpg 100%[===================>]   3.17M   738KB/s    in 5.0s    \n",
            "\n",
            "2022-03-03 12:08:28 (654 KB/s) - 20201112_072342.jpg saved [3320786/3320786]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6ByI6uXJrn9z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}