{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62e9f84d",
   "metadata": {},
   "source": [
    "# Lab14 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6e60fe",
   "metadata": {},
   "source": [
    "### st122314"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d0d32d",
   "metadata": {},
   "source": [
    "# MuZero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e971c89f",
   "metadata": {},
   "source": [
    "### 1. MuZero function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa17b3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def muzero(config: MuZeroConfig):\n",
    "    storage = SharedStorage()\n",
    "    replay_buffer = ReplayBuffer(config)\n",
    "\n",
    "    for _ in range(config.num_actors):\n",
    "        launch_job(run_selfplay, config, storage, replay_buffer)\n",
    "\n",
    "    train_network(config, storage, replay_buffer)\n",
    "\n",
    "    return storage.latest_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdd3505",
   "metadata": {},
   "source": [
    "The entry point functiion muzero is passed a MuZeroConfig object, which stores important inforamtion about parameter settings such as the action_space_size(number of possible actions) and num_actors (the number of parallel game simulations to run).\n",
    "There are two independent parts to the MuZero algorithm, self-play (creating game data) and training (producing improved versions of the neural network models). The SharedStorage and ReplayBuffer objects can be accessed by both halves of the algorithm in order to store neural network versions and game data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98ce9a9",
   "metadata": {},
   "source": [
    "### 2. Shared Storage and the Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d4e5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedStorage(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._networks = {}\n",
    "\n",
    "    def latest_network(self) -> Network:\n",
    "        if self._networks:\n",
    "            return self._networks[max(self._networks.keys())]\n",
    "        else:\n",
    "            # policy -> uniform, value -> 0, reward -> 0\n",
    "            return make_uniform_network()\n",
    "\n",
    "    def save_network(self, step: int, network: Network):\n",
    "        self._networks[step] = network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16da05d",
   "metadata": {},
   "source": [
    "The SharedStorage object contains methods for saving a version of the neural network and retrieving the latest neural network from the store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca578c17",
   "metadata": {},
   "source": [
    "### 3. Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092c1167",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, config: MuZeroConfig):\n",
    "        self.window_size = config.window_size\n",
    "        self.batch_size = config.batch_size\n",
    "        self.buffer = []\n",
    "\n",
    "    def save_game(self, game):\n",
    "        if len(self.buffer) > self.window_size:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append(game)\n",
    "\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e115a0aa",
   "metadata": {},
   "source": [
    "The ReplayBuffer stores data from previous games. The ReplayBuffer class contains a sample_batch method to sample a batch of observations from the buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3144a71",
   "metadata": {},
   "source": [
    "### 4. Self-play(run-selfplay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58067a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_selfplay(config: MuZeroConfig, storage: SharedStorage,\n",
    "                 replay_buffer: ReplayBuffer):\n",
    "    while True:\n",
    "        network = storage.latest_network()\n",
    "        game = play_game(config, network)\n",
    "        replay_buffer.save_game(game)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c54bbc",
   "metadata": {},
   "source": [
    "The method plays thousands of games against itself. In the process, the games are saved to a buffer, and then training utilizes the data from those games. This step is the same as AlphaZero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1642aa0e",
   "metadata": {},
   "source": [
    "### 5. Monte Carlo tree search(MCTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e496796",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkOutput(typing.NamedTuple):\n",
    "    value: float\n",
    "    reward: float\n",
    "    policy_logits: Dict[Action, float]\n",
    "    hidden_state: List[float]\n",
    "\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def initial_inference(self, image) -> NetworkOutput:\n",
    "        # representation + prediction function\n",
    "        return NetworkOutput(0, 0, {}, [])\n",
    "\n",
    "    def recurrent_inference(self, hidden_state, action) -> NetworkOutput:\n",
    "        # dynamics + prediction function\n",
    "        return NetworkOutput(0, 0, {}, [])\n",
    "\n",
    "    def get_weights(self):\n",
    "        # Returns the weights of this network.\n",
    "        return []\n",
    "\n",
    "    def training_steps(self) -> int:\n",
    "        # How many steps / batches the network has been trained for.\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314b4994",
   "metadata": {},
   "source": [
    "In terms of the pseudocode, there are two key inference functions used to move through the MCTS tree making predictions:\n",
    "\n",
    "- initial_inference for the current state. Calls h followed by f.\n",
    "- recurrent_inference for moving between states inside the MCTS tree. Calls g followed by f."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22738ded",
   "metadata": {},
   "source": [
    "### 6. Playing a game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37261326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(config: MuZeroConfig, network: Network) -> Game:\n",
    "    game = config.new_game()\n",
    "\n",
    "    while not game.terminal() and len(game.history) < config.max_moves:\n",
    "        # At the root of the search tree we use the representation function to\n",
    "        # obtain a hidden state given the current observation.\n",
    "        root = Node(0)\n",
    "        current_observation = game.make_image(-1)\n",
    "        expand_node(root, game.to_play(), game.legal_actions(),\n",
    "                    network.initial_inference(current_observation))\n",
    "        add_exploration_noise(config, root)\n",
    "\n",
    "        # We then run a Monte Carlo Tree Search using only action sequences and the\n",
    "        # model learned by the network.\n",
    "        run_mcts(config, root, game.action_history(), network)\n",
    "        action = select_action(config, len(game.history), root, network)\n",
    "        game.apply(action)\n",
    "        game.store_search_statistics(root)\n",
    "    return game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dce16e",
   "metadata": {},
   "source": [
    "A game is a loop. The game ends when a terminal condition is met or the maximum number of moves is reached.\n",
    "\n",
    "When a new game is started, MCTS must be started over at the root node.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9813409",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "\n",
    "    def __init__(self, prior: float):\n",
    "        self.visit_count = 0\n",
    "        self.to_play = -1\n",
    "        self.prior = prior\n",
    "        self.value_sum = 0\n",
    "        self.children = {}\n",
    "        self.hidden_state = None\n",
    "        self.reward = 0\n",
    "\n",
    "    def expanded(self) -> bool:\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def value(self) -> float:\n",
    "        if self.visit_count == 0:\n",
    "            return 0\n",
    "        return self.value_sum / self.visit_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9084fcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_observation = game.make_image(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214b4080",
   "metadata": {},
   "source": [
    "Then we request the game to return the current observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7121968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_node(node: Node, to_play: Player, actions: List[Action],\n",
    "                network_output: NetworkOutput):\n",
    "    node.to_play = to_play\n",
    "    node.hidden_state = network_output.hidden_state\n",
    "    node.reward = network_output.reward\n",
    "    policy = {a: math.exp(network_output.policy_logits[a]) for a in actions}\n",
    "    policy_sum = sum(policy.values())\n",
    "    for action, p in policy.items():\n",
    "        node.children[action] = Node(p / policy_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942e218e",
   "metadata": {},
   "source": [
    "Next, we expand the root node using the known legal actions provided by the game and the inference about the current observation provided by the initial_inference function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5305fee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "expand_node(root, game.to_play(), game.legal_actions(), network.initial_inference(current_observation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d7e681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_exploration_noise(config: MuZeroConfig, node: Node):\n",
    "    actions = list(node.children.keys())\n",
    "    noise = numpy.random.dirichlet([config.root_dirichlet_alpha] * len(actions))\n",
    "    frac = config.root_exploration_fraction\n",
    "    for a, n in zip(actions, noise):\n",
    "        node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c227a75",
   "metadata": {},
   "source": [
    "We need to add exploration noise to the root node actions, to ensure that MCTS explores a range of possible actions rather than only exploring the action which it currently believes to be optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cebe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_exploration_noise(config, root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfa7937",
   "metadata": {},
   "source": [
    "### 7. MCTS run function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1449303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_child(config: MuZeroConfig, node: Node,\n",
    "                 min_max_stats: MinMaxStats):\n",
    "    _, action, child = max(\n",
    "        (ucb_score(config, node, child, min_max_stats), action,\n",
    "         child) for action, child in node.children.items())\n",
    "    return action, child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b43e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate(search_path: List[Node], value: float, to_play: Player,\n",
    "                  discount: float, min_max_stats: MinMaxStats):\n",
    "    for node in search_path:\n",
    "        node.value_sum += value if node.to_play == to_play else -value\n",
    "        node.visit_count += 1\n",
    "        min_max_stats.update(node.value())\n",
    "\n",
    "        value = node.reward + discount * value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b117806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mcts(config: MuZeroConfig, root: Node, action_history: ActionHistory,\n",
    "             network: Network):\n",
    "    min_max_stats = MinMaxStats(config.known_bounds)\n",
    "\n",
    "    for _ in range(config.num_simulations):\n",
    "        history = action_history.clone()\n",
    "        node = root\n",
    "        search_path = [node]\n",
    "\n",
    "        while node.expanded():\n",
    "            action, node = select_child(config, node, min_max_stats)\n",
    "            history.add_action(action)\n",
    "            search_path.append(node)\n",
    "\n",
    "        # Inside the search tree we use the dynamics function to obtain the next\n",
    "        # hidden state given an action and the previous hidden state.\n",
    "        parent = search_path[-2]\n",
    "        network_output = network.recurrent_inference(parent.hidden_state,\n",
    "                                                     history.last_action())\n",
    "        expand_node(node, history.to_play(), history.action_space(), network_output)\n",
    "\n",
    "        backpropagate(search_path, network_output.value, history.to_play(),\n",
    "                      config.discount, min_max_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23ff3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_mcts(config, root, game.action_history(), network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30b0f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(config: MuZeroConfig, num_moves: int, node: Node,\n",
    "                  network: Network):\n",
    "    visit_counts = [\n",
    "        (child.visit_count, action) for action, child in node.children.items()\n",
    "    ]\n",
    "    t = config.visit_softmax_temperature_fn(\n",
    "        num_moves=num_moves, training_steps=network.training_steps())\n",
    "    _, action = softmax_sample(visit_counts, t)\n",
    "    return action\n",
    "\n",
    "def visit_softmax_temperature(num_moves, training_steps):\n",
    "    if num_moves < 30:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0  # Play according to the max."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6335bd",
   "metadata": {},
   "source": [
    "### 8. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ae53fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(config: MuZeroConfig, storage: SharedStorage,\n",
    "                  replay_buffer: ReplayBuffer):\n",
    "    network = Network()\n",
    "    learning_rate = config.lr_init * config.lr_decay_rate**(\n",
    "        tf.train.get_global_step() / config.lr_decay_steps)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, config.momentum)\n",
    "\n",
    "    for i in range(config.training_steps):\n",
    "        if i % config.checkpoint_interval == 0:\n",
    "            storage.save_network(i, network)\n",
    "        batch = replay_buffer.sample_batch(config.num_unroll_steps, config.td_steps)\n",
    "        update_weights(optimizer, network, batch, config.weight_decay)\n",
    "    storage.save_network(config.training_steps, network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba76f77",
   "metadata": {},
   "source": [
    "It first creates a new Network object (that stores randomly initialised instances of MuZero’s three neural networks) and sets the learning rate to decay based on the number of training steps that have been completed. We also create the gradient descent optimiser that will calculate the magnitude and direction of the weight updates at each training step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435ce0b5",
   "metadata": {},
   "source": [
    "### 9. MuZero Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a9d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(optimizer: tf.train.Optimizer, network: Network, batch,\n",
    "                   weight_decay: float):\n",
    "    loss = 0\n",
    "    for image, actions, targets in batch:\n",
    "        # Initial step, from the real observation.\n",
    "        value, reward, policy_logits, hidden_state = network.initial_inference(\n",
    "            image)\n",
    "        predictions = [(1.0, value, reward, policy_logits)]\n",
    "\n",
    "    # Recurrent steps, from action and previous hidden state.\n",
    "    for action in actions:\n",
    "        value, reward, policy_logits, hidden_state = network.recurrent_inference(\n",
    "          hidden_state, action)\n",
    "        predictions.append((1.0 / len(actions), value, reward, policy_logits))\n",
    "\n",
    "        hidden_state = tf.scale_gradient(hidden_state, 0.5)\n",
    "\n",
    "    for prediction, target in zip(predictions, targets):\n",
    "        gradient_scale, value, reward, policy_logits = prediction\n",
    "        target_value, target_reward, target_policy = target\n",
    "\n",
    "        l = (\n",
    "          scalar_loss(value, target_value) +\n",
    "          scalar_loss(reward, target_reward) +\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "              logits=policy_logits, labels=target_policy))\n",
    "\n",
    "        loss += tf.scale_gradient(l, gradient_scale)\n",
    "\n",
    "    for weights in network.get_weights():\n",
    "        loss += weight_decay * tf.nn.l2_loss(weights)\n",
    "\n",
    "    optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e4c35f",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7df5b57",
   "metadata": {},
   "source": [
    "\n",
    "In this lab, I have leant about MuZero and AlphaZeo and Evaluaiotn of MiZero. AlphaZero (AZ) is a more generalized variant of the AlphaGo Zero (AGZ) algorithm, and is able to play shogi and chess as well as Go while MuZero (MZ) combines the AlphaZero (AZ) algorithm's high-performance planning with model-free reinforcement learning methodologies. The combination allows for more effective training in traditional planning regimes like Go, as well as domains with significantly more complex inputs at each level, such visual video games.\n",
    "\n",
    "Form my understanding, the evaluation of Muzero are the following.\n",
    "- AlphaGo becomes the first program to mater Go using meural networks and tree seach. (Jan 2016, Nature)\n",
    "- AlphaGo Zero learns to play completely on its own, without human knowlwdege. (Oct 2017, Nature)\n",
    "- AlphaZero maters three perfect imforamtion games using a single algorithm for all games. (Dec 2018, Science)\n",
    "- MuZero learns the rules the game, allowing it to also "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c404a3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f89c5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25a6f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f88db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e3133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
