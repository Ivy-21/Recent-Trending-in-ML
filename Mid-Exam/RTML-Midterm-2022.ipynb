{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RTML Midterm 2022\n",
    "\n",
    "## Question 1 (20 points)\n",
    "\n",
    "In Labs 04 and 05, you developed your own PyTorch implementations of YOLOv4 and YOLOR.\n",
    "Download the image at http://www.cs.ait.ac.th/~mdailey/ait-orientation.jpg and run it\n",
    "through your YOLOv4 and YOLOR models. Provide your source code to load the model, image,\n",
    "get the result, and display the result here. Display the resulting bounding boxes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "#import cv2 \n",
    "\n",
    "def unique(tensor):\n",
    "    #tensor_np = tensor.cpu().numpy()\n",
    "    tensor_np = tensor.detach().cpu().numpy()\n",
    "    unique_np = np.unique(tensor_np)\n",
    "    unique_tensor = torch.from_numpy(unique_np)\n",
    "    \n",
    "    tensor_res = tensor.new(unique_tensor.shape)\n",
    "    tensor_res.copy_(unique_tensor)\n",
    "    return tensor_res\n",
    "\n",
    "\n",
    "def bbox_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Returns the IoU of two bounding boxes \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    #Get the coordinates of bounding boxes\n",
    "    b1_x1, b1_y1, b1_x2, b1_y2 = box1[:,0], box1[:,1], box1[:,2], box1[:,3]\n",
    "    b2_x1, b2_y1, b2_x2, b2_y2 = box2[:,0], box2[:,1], box2[:,2], box2[:,3]\n",
    "    \n",
    "    #get the corrdinates of the intersection rectangle\n",
    "    inter_rect_x1 =  torch.max(b1_x1, b2_x1)\n",
    "    inter_rect_y1 =  torch.max(b1_y1, b2_y1)\n",
    "    inter_rect_x2 =  torch.min(b1_x2, b2_x2)\n",
    "    inter_rect_y2 =  torch.min(b1_y2, b2_y2)\n",
    "    \n",
    "    #Intersection area\n",
    "    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * torch.clamp(inter_rect_y2 - inter_rect_y1 + 1, min=0)\n",
    "\n",
    "    #Union Area\n",
    "    b1_area = (b1_x2 - b1_x1 + 1)*(b1_y2 - b1_y1 + 1)\n",
    "    b2_area = (b2_x2 - b2_x1 + 1)*(b2_y2 - b2_y1 + 1)\n",
    "    \n",
    "    iou = inter_area / (b1_area + b2_area - inter_area)\n",
    "    \n",
    "    return iou\n",
    "\n",
    "def predict_transform(prediction, inp_dim, anchors, num_classes, CUDA = True):\n",
    "\n",
    "    \n",
    "    batch_size = prediction.size(0)\n",
    "    stride =  inp_dim // prediction.size(2)\n",
    "    grid_size = inp_dim // stride\n",
    "    bbox_attrs = 5 + num_classes\n",
    "    num_anchors = len(anchors)\n",
    "    \n",
    "    prediction = prediction.view(batch_size, bbox_attrs*num_anchors, grid_size*grid_size)\n",
    "    prediction = prediction.transpose(1,2).contiguous()\n",
    "    prediction = prediction.view(batch_size, grid_size*grid_size*num_anchors, bbox_attrs)\n",
    "    anchors = [(a[0]/stride, a[1]/stride) for a in anchors]\n",
    "\n",
    "    #Sigmoid the  centre_X, centre_Y. and object confidencce\n",
    "    prediction[:,:,0] = torch.sigmoid(prediction[:,:,0])\n",
    "    prediction[:,:,1] = torch.sigmoid(prediction[:,:,1])\n",
    "    prediction[:,:,4] = torch.sigmoid(prediction[:,:,4])\n",
    "    \n",
    "    #Add the center offsets\n",
    "    grid = np.arange(grid_size)\n",
    "    a,b = np.meshgrid(grid, grid)\n",
    "\n",
    "    x_offset = torch.FloatTensor(a).view(-1,1)\n",
    "    y_offset = torch.FloatTensor(b).view(-1,1)\n",
    "\n",
    "    if CUDA:\n",
    "        x_offset = x_offset.cuda()\n",
    "        y_offset = y_offset.cuda()\n",
    "\n",
    "    x_y_offset = torch.cat((x_offset, y_offset), 1).repeat(1,num_anchors).view(-1,2).unsqueeze(0)\n",
    "\n",
    "    prediction[:,:,:2] += x_y_offset\n",
    "\n",
    "    #log space transform height and the width\n",
    "    anchors = torch.FloatTensor(anchors)\n",
    "\n",
    "    if CUDA:\n",
    "        anchors = anchors.cuda()\n",
    "\n",
    "    anchors = anchors.repeat(grid_size*grid_size, 1).unsqueeze(0)\n",
    "    prediction[:,:,2:4] = torch.exp(prediction[:,:,2:4])*anchors\n",
    "    \n",
    "    prediction[:,:,5: 5 + num_classes] = torch.sigmoid((prediction[:,:, 5 : 5 + num_classes]))\n",
    "\n",
    "    prediction[:,:,:4] *= stride\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "def write_results(prediction, confidence, num_classes, nms_conf = 0.4):\n",
    "    conf_mask = (prediction[:,:,4] > confidence).float().unsqueeze(2)\n",
    "    prediction = prediction*conf_mask\n",
    "    \n",
    "    box_corner = prediction.new(prediction.shape)\n",
    "    box_corner[:,:,0] = (prediction[:,:,0] - prediction[:,:,2]/2)\n",
    "    box_corner[:,:,1] = (prediction[:,:,1] - prediction[:,:,3]/2)\n",
    "    box_corner[:,:,2] = (prediction[:,:,0] + prediction[:,:,2]/2) \n",
    "    box_corner[:,:,3] = (prediction[:,:,1] + prediction[:,:,3]/2)\n",
    "    prediction[:,:,:4] = box_corner[:,:,:4]\n",
    "    \n",
    "    batch_size = prediction.size(0)\n",
    "\n",
    "    write = False\n",
    "    \n",
    "\n",
    "\n",
    "    for ind in range(batch_size):\n",
    "        image_pred = prediction[ind]          #image Tensor\n",
    "       #confidence threshholding \n",
    "       #NMS\n",
    "    \n",
    "        max_conf, max_conf_score = torch.max(image_pred[:,5:5+ num_classes], 1)\n",
    "        max_conf = max_conf.float().unsqueeze(1)\n",
    "        max_conf_score = max_conf_score.float().unsqueeze(1)\n",
    "        seq = (image_pred[:,:5], max_conf, max_conf_score)\n",
    "        image_pred = torch.cat(seq, 1)\n",
    "        \n",
    "        non_zero_ind =  (torch.nonzero(image_pred[:,4]))\n",
    "        try:\n",
    "            image_pred_ = image_pred[non_zero_ind.squeeze(),:].view(-1,7)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        if image_pred_.shape[0] == 0:\n",
    "            continue       \n",
    "#        \n",
    "  \n",
    "        #Get the various classes detected in the image\n",
    "        img_classes = unique(image_pred_[:,-1])  # -1 index holds the class index\n",
    "        \n",
    "        \n",
    "        for cls in img_classes:\n",
    "            #perform NMS\n",
    "\n",
    "        \n",
    "            #get the detections with one particular class\n",
    "            cls_mask = image_pred_*(image_pred_[:,-1] == cls).float().unsqueeze(1)\n",
    "            class_mask_ind = torch.nonzero(cls_mask[:,-2]).squeeze()\n",
    "            image_pred_class = image_pred_[class_mask_ind].view(-1,7)\n",
    "            \n",
    "            #sort the detections such that the entry with the maximum objectness\n",
    "            #confidence is at the top\n",
    "            conf_sort_index = torch.sort(image_pred_class[:,4], descending = True )[1]\n",
    "            image_pred_class = image_pred_class[conf_sort_index]\n",
    "            idx = image_pred_class.size(0)   #Number of detections\n",
    "            \n",
    "            for i in range(idx):\n",
    "                #Get the IOUs of all boxes that come after the one we are looking at \n",
    "                #in the loop\n",
    "                try:\n",
    "                    ious = bbox_iou(image_pred_class[i].unsqueeze(0), image_pred_class[i+1:])\n",
    "                except ValueError:\n",
    "                    break\n",
    "            \n",
    "                except IndexError:\n",
    "                    break\n",
    "            \n",
    "                #Zero out all the detections that have IoU > treshhold\n",
    "                iou_mask = (ious < nms_conf).float().unsqueeze(1)\n",
    "                image_pred_class[i+1:] *= iou_mask       \n",
    "            \n",
    "                #Remove the non-zero entries\n",
    "                non_zero_ind = torch.nonzero(image_pred_class[:,4]).squeeze()\n",
    "                image_pred_class = image_pred_class[non_zero_ind].view(-1,7)\n",
    "                \n",
    "            batch_ind = image_pred_class.new(image_pred_class.size(0), 1).fill_(ind)      #Repeat the batch_id for as many detections of the class cls in the image\n",
    "            seq = batch_ind, image_pred_class\n",
    "            \n",
    "            if not write:\n",
    "                output = torch.cat(seq,1)\n",
    "                write = True\n",
    "            else:\n",
    "                out = torch.cat(seq,1)\n",
    "                output = torch.cat((output,out))\n",
    "\n",
    "    try:\n",
    "        return output\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def letterbox_image(img, inp_dim):\n",
    "    '''resize image with unchanged aspect ratio using padding'''\n",
    "    img_w, img_h = img.shape[1], img.shape[0]\n",
    "    w, h = inp_dim\n",
    "    new_w = int(img_w * min(w/img_w, h/img_h))\n",
    "    new_h = int(img_h * min(w/img_w, h/img_h))\n",
    "    resized_image = cv2.resize(img, (new_w,new_h), interpolation = cv2.INTER_CUBIC)\n",
    "    \n",
    "    canvas = np.full((inp_dim[1], inp_dim[0], 3), 128)\n",
    "\n",
    "    canvas[(h-new_h)//2:(h-new_h)//2 + new_h,(w-new_w)//2:(w-new_w)//2 + new_w,  :] = resized_image\n",
    "    \n",
    "    return canvas\n",
    "\n",
    "def prep_image(img, inp_dim):\n",
    "    \"\"\"\n",
    "    Prepare image for inputting to the neural network. \n",
    "    \n",
    "    Returns a Variable \n",
    "    \"\"\"\n",
    "    img = (letterbox_image(img, (inp_dim, inp_dim)))\n",
    "    img = img[:,:,::-1].transpose((2,0,1)).copy()\n",
    "    img = torch.from_numpy(img).float().div(255.0).unsqueeze(0)\n",
    "    return img\n",
    "\n",
    "def load_classes(namesfile):\n",
    "    fp = open(namesfile, \"r\")\n",
    "    names = fp.read().split(\"\\n\")[:-1]\n",
    "    return names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import tanh\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x * tanh(F.softplus(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "#from util import * \n",
    "#from mish import Mish\n",
    "\n",
    "\n",
    "\n",
    "def get_test_input():\n",
    "    img = cv2.imread(\"ait-orientation.jpeg\")\n",
    "    img = cv2.resize(img, (416,416))          #Resize to the input dimension\n",
    "    img_ =  img[:,:,::-1].transpose((2,0,1))  # BGR -> RGB | H X W C -> C X H X W \n",
    "    img_ = img_[np.newaxis,:,:,:]/255.0       #Add a channel at 0 (for batch) | Normalise\n",
    "    img_ = torch.from_numpy(img_).float()     #Convert to float\n",
    "    img_ = Variable(img_)                     # Convert to Variable\n",
    "    return img_\n",
    "\n",
    "def parse_cfg(cfgfile):\n",
    "    \"\"\"\n",
    "    Takes a configuration file\n",
    "    \n",
    "    Returns a list of blocks. Each blocks describes a block in the neural\n",
    "    network to be built. Block is represented as a dictionary in the list\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    file = open(cfgfile, 'r')\n",
    "    lines = file.read().split('\\n')                        # store the lines in a list\n",
    "    lines = [x for x in lines if len(x) > 0]               # get read of the empty lines \n",
    "    lines = [x for x in lines if x[0] != '#']              # get rid of comments\n",
    "    lines = [x.rstrip().lstrip() for x in lines]           # get rid of fringe whitespaces\n",
    "    \n",
    "    block = {}\n",
    "    blocks = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if line[0] == \"[\":               # This marks the start of a new block\n",
    "            if len(block) != 0:          # If block is not empty, implies it is storing values of previous block.\n",
    "                blocks.append(block)     # add it the blocks list\n",
    "                block = {}               # re-init the block\n",
    "            block[\"type\"] = line[1:-1].rstrip()     \n",
    "        else:\n",
    "            key,value = line.split(\"=\") \n",
    "            block[key.rstrip()] = value.lstrip()\n",
    "    blocks.append(block)\n",
    "    \n",
    "    return blocks\n",
    "\n",
    "\n",
    "class EmptyLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmptyLayer, self).__init__()\n",
    "        \n",
    "\n",
    "class DetectionLayer(nn.Module):\n",
    "    def __init__(self, anchors):\n",
    "        super(DetectionLayer, self).__init__()\n",
    "        self.anchors = anchors\n",
    "\n",
    "\n",
    "def create_modules(blocks):\n",
    "    net_info = blocks[0]     #Captures the information about the input and pre-processing    \n",
    "    module_list = nn.ModuleList()\n",
    "    prev_filters = 3\n",
    "    output_filters = []\n",
    "    \n",
    "    for index, x in enumerate(blocks[1:]):\n",
    "        module = nn.Sequential()\n",
    "    \n",
    "        #check the type of block\n",
    "        #create a new module for the block\n",
    "        #append to module_list\n",
    "        \n",
    "        #If it's a convolutional layer\n",
    "        if (x[\"type\"] == \"convolutional\"):\n",
    "            #Get the info about the layer\n",
    "            activation = x[\"activation\"]\n",
    "            try:\n",
    "                batch_normalize = int(x[\"batch_normalize\"])\n",
    "                bias = False\n",
    "            except:\n",
    "                batch_normalize = 0\n",
    "                bias = True\n",
    "        \n",
    "            filters= int(x[\"filters\"])\n",
    "            padding = int(x[\"pad\"])\n",
    "            kernel_size = int(x[\"size\"])\n",
    "            stride = int(x[\"stride\"])\n",
    "        \n",
    "            if padding:\n",
    "                pad = (kernel_size - 1) // 2\n",
    "            else:\n",
    "                pad = 0\n",
    "        \n",
    "            #Add the convolutional layer\n",
    "            conv = nn.Conv2d(prev_filters, filters, kernel_size, stride, pad, bias = bias)\n",
    "            module.add_module(\"conv_{0}\".format(index), conv)\n",
    "        \n",
    "            #Add the Batch Norm Layer\n",
    "            if batch_normalize:\n",
    "                bn = nn.BatchNorm2d(filters)\n",
    "                module.add_module(\"batch_norm_{0}\".format(index), bn)\n",
    "        \n",
    "            #Check the activation. \n",
    "            #It is either Linear or a Leaky ReLU for YOLO\n",
    "            if activation == \"leaky\":\n",
    "                activn = nn.LeakyReLU(0.1, inplace = True)\n",
    "                module.add_module(\"leaky_{0}\".format(index), activn)\n",
    "                \n",
    "            #''' Mish activation modification here '''\n",
    "            elif activation == \"mish\":\n",
    "                activn = Mish()\n",
    "                module.add_module(\"mish_{0}\".format(index), activn)\n",
    "\n",
    "        \n",
    "        #If it's an upsampling layer\n",
    "        #We use Bilinear2dUpsampling\n",
    "        elif (x[\"type\"] == \"upsample\"):\n",
    "            stride = int(x[\"stride\"])\n",
    "            upsample = nn.Upsample(scale_factor = 2, mode = \"nearest\")\n",
    "            module.add_module(\"upsample_{}\".format(index), upsample)\n",
    "            \n",
    "        #''' route layermodification here '''\n",
    "        elif (x[\"type\"] == \"route\"):\n",
    "            x[\"layers\"] = x[\"layers\"].split(',')\n",
    "            filters = 0\n",
    "\n",
    "            for i in range(len(x[\"layers\"])):\n",
    "                pointer = int(x[\"layers\"][i])\n",
    "                if  pointer > 0:\n",
    "                    filters += output_filters[pointer]\n",
    "                else:\n",
    "                    filters += output_filters[index + pointer]\n",
    "\n",
    "            route = EmptyLayer()\n",
    "            module.add_module(\"route_{0}\".format(index), route)\n",
    "    \n",
    "        #shortcut corresponds to skip connection\n",
    "        elif x[\"type\"] == \"shortcut\":\n",
    "            shortcut = EmptyLayer()\n",
    "            module.add_module(\"shortcut_{}\".format(index), shortcut)\n",
    "            \n",
    "        #Yolo is the detection layer\n",
    "        elif x[\"type\"] == \"yolo\":\n",
    "            mask = x[\"mask\"].split(\",\")\n",
    "            mask = [int(x) for x in mask]\n",
    "    \n",
    "            anchors = x[\"anchors\"].split(\",\")\n",
    "            anchors = [int(a) for a in anchors]\n",
    "            anchors = [(anchors[i], anchors[i+1]) for i in range(0, len(anchors),2)]\n",
    "            anchors = [anchors[i] for i in mask]\n",
    "    \n",
    "            detection = DetectionLayer(anchors)\n",
    "            module.add_module(\"Detection_{}\".format(index), detection)\n",
    "        \n",
    "       # ''' Max pooling layer modification here '''\n",
    "        elif x[\"type\"] == \"maxpool\":\n",
    "            stride = int(x[\"stride\"])\n",
    "            size = int(x[\"size\"])\n",
    "            max_pool = nn.MaxPool2d(size, stride, padding=size // 2)\n",
    "            module.add_module(\"maxpool_{}\".format(index), max_pool)\n",
    "                              \n",
    "        module_list.append(module)\n",
    "        prev_filters = filters\n",
    "        output_filters.append(filters)\n",
    "        \n",
    "    return (net_info, module_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MyDarknet(nn.Module):\n",
    "    def __init__(self, cfgfile):\n",
    "        super(MyDarknet, self).__init__()\n",
    "        # load the config file and create our model\n",
    "        self.blocks = parse_cfg(cfgfile)\n",
    "        self.net_info, self.module_list = create_modules(self.blocks)\n",
    "        \n",
    "    def forward(self, x, CUDA:bool):\n",
    "        modules = self.blocks[1:]\n",
    "        outputs = {}   #We cache the outputs for the route layer\n",
    "        \n",
    "        write = 0\n",
    "        # run forward propagation. Follow the instruction from dictionary modules\n",
    "        for i, module in enumerate(modules):        \n",
    "            module_type = (module[\"type\"])\n",
    "            \n",
    "            if module_type == \"convolutional\" or module_type == \"upsample\":\n",
    "                # do convolutional network\n",
    "                x = self.module_list[i](x)\n",
    "    \n",
    "            elif module_type == \"route\":\n",
    "                # concat layers\n",
    "                layers = module[\"layers\"]\n",
    "                layers = [int(a) for a in layers]\n",
    "    \n",
    "                if (layers[0]) > 0:\n",
    "                    layers[0] = layers[0] - i\n",
    "    \n",
    "                if len(layers) == 1:\n",
    "                    x = outputs[i + (layers[0])]\n",
    "    \n",
    "                else:\n",
    "                    if (layers[1]) > 0:\n",
    "                        layers[1] = layers[1] - i\n",
    "    \n",
    "                    map1 = outputs[i + layers[0]]\n",
    "                    map2 = outputs[i + layers[1]]\n",
    "                    x = torch.cat((map1, map2), 1)\n",
    "                \n",
    "    \n",
    "            elif  module_type == \"shortcut\":\n",
    "                from_ = int(module[\"from\"])\n",
    "                # residual network\n",
    "                x = outputs[i-1] + outputs[i+from_]\n",
    "    \n",
    "            elif module_type == 'yolo':        \n",
    "                anchors = self.module_list[i][0].anchors\n",
    "                #Get the input dimensions\n",
    "                inp_dim = int (self.net_info[\"height\"])\n",
    "        \n",
    "                #Get the number of classes\n",
    "                num_classes = int (module[\"classes\"])\n",
    "        \n",
    "                #Transform \n",
    "                #x = x.data\n",
    "                # predict_transform is in util.py\n",
    "                x = predict_transform(x, inp_dim, anchors, num_classes, CUDA)\n",
    "                if not write:              #if no collector has been intialised. \n",
    "                    detections = x\n",
    "                    write = 1\n",
    "        \n",
    "                else:       \n",
    "                    detections = torch.cat((detections, x), 1)\n",
    "        \n",
    "            outputs[i] = x\n",
    "        \n",
    "        return detections\n",
    "\n",
    "\n",
    "    def load_weights(self, weightfile, backbone_only = False):\n",
    "        '''\n",
    "        Load pretrained weight\n",
    "        '''\n",
    "        #Open the weights file\n",
    "        fp = open(weightfile, \"rb\")\n",
    "    \n",
    "        #The first 5 values are header information \n",
    "        # 1. Major version number\n",
    "        # 2. Minor Version Number\n",
    "        # 3. Subversion number \n",
    "        # 4,5. Images seen by the network (during training)\n",
    "        header = np.fromfile(fp, dtype = np.int32, count = 5)\n",
    "        self.header = torch.from_numpy(header)\n",
    "        self.seen = self.header[3]   \n",
    "        \n",
    "        weights = np.fromfile(fp, dtype = np.float32)\n",
    "        \n",
    "        ptr = 0\n",
    "        for i in range(len(self.module_list)):\n",
    "            \n",
    "            if i>104 and backbone_only:\n",
    "                break\n",
    "            \n",
    "            module_type = self.blocks[i + 1][\"type\"]\n",
    "    \n",
    "            #If module_type is convolutional load weights\n",
    "            #Otherwise ignore.\n",
    "            \n",
    "            if module_type == \"convolutional\":\n",
    "                model = self.module_list[i]\n",
    "                try:\n",
    "                    batch_normalize = int(self.blocks[i+1][\"batch_normalize\"])\n",
    "                except:\n",
    "                    batch_normalize = 0\n",
    "            \n",
    "                conv = model[0]\n",
    "                \n",
    "                \n",
    "                if (batch_normalize):\n",
    "                    bn = model[1]\n",
    "        \n",
    "                    #Get the number of weights of Batch Norm Layer\n",
    "                    num_bn_biases = bn.bias.numel()\n",
    "        \n",
    "                    #Load the weights\n",
    "                    bn_biases = torch.from_numpy(weights[ptr:ptr + num_bn_biases])\n",
    "                    ptr += num_bn_biases\n",
    "        \n",
    "                    bn_weights = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
    "                    ptr  += num_bn_biases\n",
    "        \n",
    "                    bn_running_mean = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
    "                    ptr  += num_bn_biases\n",
    "        \n",
    "                    bn_running_var = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
    "                    ptr  += num_bn_biases\n",
    "        \n",
    "                    #Cast the loaded weights into dims of model weights. \n",
    "                    bn_biases = bn_biases.view_as(bn.bias.data)\n",
    "                    bn_weights = bn_weights.view_as(bn.weight.data)\n",
    "                    bn_running_mean = bn_running_mean.view_as(bn.running_mean)\n",
    "                    bn_running_var = bn_running_var.view_as(bn.running_var)\n",
    "        \n",
    "                    #Copy the data to model\n",
    "                    bn.bias.data.copy_(bn_biases)\n",
    "                    bn.weight.data.copy_(bn_weights)\n",
    "                    bn.running_mean.copy_(bn_running_mean)\n",
    "                    bn.running_var.copy_(bn_running_var)\n",
    "                \n",
    "                else:\n",
    "                    #Number of biases\n",
    "                    num_biases = conv.bias.numel()\n",
    "                \n",
    "                    #Load the weights\n",
    "                    conv_biases = torch.from_numpy(weights[ptr: ptr + num_biases])\n",
    "                    ptr = ptr + num_biases\n",
    "                \n",
    "                    #reshape the loaded weights according to the dims of the model weights\n",
    "                    conv_biases = conv_biases.view_as(conv.bias.data)\n",
    "                \n",
    "                    #Finally copy the data\n",
    "                    conv.bias.data.copy_(conv_biases)\n",
    "                    \n",
    "                #Let us load the weights for the Convolutional layers\n",
    "                num_weights = conv.weight.numel()\n",
    "                \n",
    "                #Do the same as above for weights\n",
    "                conv_weights = torch.from_numpy(weights[ptr:ptr+num_weights])\n",
    "                ptr = ptr + num_weights\n",
    "                \n",
    "                conv_weights = conv_weights.view_as(conv.weight.data)\n",
    "                conv.weight.data.copy_(conv_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://pjreddie.com/media/files/yolov4.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/cfg/yolov4.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOLOv4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1024, 512, 3, 3]' is invalid for input of size 2552319",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_114/173419776.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#Create YOLOv3 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyDarknet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yolov4.cfg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yolov4.weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#model = MyDarknet(\"cfg/yolov3.cfg\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_114/2710790522.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, weightfile, backbone_only)\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mptr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0mconv_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1024, 512, 3, 3]' is invalid for input of size 2552319"
     ]
    }
   ],
   "source": [
    "# Code to load model, image, and display result here\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "#import cv2\n",
    "import torch\n",
    "#from util import *\n",
    "#from darknet import MyDarknet\n",
    "\n",
    "def get_test_input():\n",
    "    img = cv2.imread(\"ait-orientation.jpeg\")\n",
    "    img = cv2.resize(img, (416,416))          #Resize to the input dimension\n",
    "    img_ =  img[:,:,::-1].transpose((2,0,1))  # BGR -> RGB | H X W C -> C X H X W \n",
    "    img_ = img_[np.newaxis,:,:,:]/255.0       #Add a channel at 0 (for batch) | Normalise\n",
    "    img_ = torch.from_numpy(img_).float()     #Convert to float\n",
    "    img_ = Variable(img_)                     # Convert to Variable\n",
    "    return img_\n",
    "\n",
    "#Create YOLOv3 model\n",
    "model = MyDarknet(\"yolov4.cfg\")\n",
    "model.load_weights(\"yolov4.weights\")\n",
    "\n",
    "#model = MyDarknet(\"cfg/yolov3.cfg\")\n",
    "inp = get_test_input()\n",
    "pred = model(inp, False)\n",
    "print (pred)\n",
    "print('Output tensor size :', pred.shape)\n",
    "\n",
    "\n",
    "result = write_results(pred, 0.5, 80, nms_conf = 0.4)\n",
    "print(result)\n",
    "\n",
    "\n",
    "def load_classes(namesfile):\n",
    "    fp = open(namesfile, \"r\")\n",
    "    names = fp.read().split(\"\\n\")[:-1]\n",
    "    return names\n",
    "\n",
    "num_classes = 91\n",
    "coco_names = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOLOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1024, 512, 3, 3]' is invalid for input of size 3925786",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_114/604548207.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#Create YOLOv3 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyDarknet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yolo.cfg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yolor_p6.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#model = MyDarknet(\"cfg/yolov3.cfg\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_114/2710790522.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, weightfile, backbone_only)\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mptr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0mconv_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1024, 512, 3, 3]' is invalid for input of size 3925786"
     ]
    }
   ],
   "source": [
    "# Code to load model, image, and display result here\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "#import cv2\n",
    "import torch\n",
    "#from util import *\n",
    "#from darknet import MyDarknet\n",
    "\n",
    "def get_test_input():\n",
    "    img = cv2.imread(\"ait-orientation.jpeg\")\n",
    "    img = cv2.resize(img, (416,416))          #Resize to the input dimension\n",
    "    img_ =  img[:,:,::-1].transpose((2,0,1))  # BGR -> RGB | H X W C -> C X H X W \n",
    "    img_ = img_[np.newaxis,:,:,:]/255.0       #Add a channel at 0 (for batch) | Normalise\n",
    "    img_ = torch.from_numpy(img_).float()     #Convert to float\n",
    "    img_ = Variable(img_)                     # Convert to Variable\n",
    "    return img_\n",
    "\n",
    "#Create YOLOv3 model\n",
    "model = MyDarknet(\"yolo.cfg\")\n",
    "model.load_weights(\"yolor_p6.pt\")\n",
    "\n",
    "#model = MyDarknet(\"cfg/yolov3.cfg\")\n",
    "inp = get_test_input()\n",
    "pred = model(inp, False)\n",
    "print (pred)\n",
    "print('Output tensor size :', pred.shape)\n",
    "\n",
    "\n",
    "result = write_results(pred, 0.5, 80, nms_conf = 0.4)\n",
    "print(result)\n",
    "\n",
    "\n",
    "def load_classes(namesfile):\n",
    "    fp = open(namesfile, \"r\")\n",
    "    names = fp.read().split(\"\\n\")[:-1]\n",
    "    return names\n",
    "\n",
    "num_classes = 91\n",
    "coco_names = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-04 03:00:41--  https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov4.cfg\n",
      "Connecting to 192.41.170.23:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 12231 (12K) [text/plain]\n",
      "Saving to: ‘yolov4.cfg’\n",
      "\n",
      "yolov4.cfg          100%[===================>]  11.94K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2022-03-04 03:00:41 (387 KB/s) - ‘yolov4.cfg’ saved [12231/12231]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!wget https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov4.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://pjreddie.com/media/files/yolov4.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading network.....\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1024, 512, 3, 3]' is invalid for input of size 2552319",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_114/631565818.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m114\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_114\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yolov4.weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Network successfully loaded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_114/7441142.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, weightfile, backbone_only)\u001b[0m\n\u001b[1;32m    319\u001b[0m                 \u001b[0mptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mptr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m                 \u001b[0mconv_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m                 \u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1024, 512, 3, 3]' is invalid for input of size 2552319"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import time\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "#import cv2 \n",
    "#from util import *\n",
    "import argparse\n",
    "import os \n",
    "import os.path as osp\n",
    "#from darknet import Darknet\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "images = \"cocoimages\"\n",
    "batch_size = 4\n",
    "confidence = 0.5\n",
    "nms_thesh = 0.4\n",
    "start = 0\n",
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "num_classes = 91\n",
    "classes = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "#Set up the neural network\n",
    "\n",
    "print(\"Loading network.....\")\n",
    "model = Darknet(\"yolov4.cfg\")\n",
    "\n",
    "# Edit Convo Layer 114\n",
    "# Here we need to edit this layer because previously the input channel to this was set as 1024 but actually this layer needs to accept the input from the concatenation of four 512-channel layers so I need to modify this layer to have input channel of 2048\n",
    "model.module_list[114].conv_114 = nn.Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "\n",
    "model.load_weights(\"yolov4.weights\")\n",
    "print(\"Network successfully loaded\")\n",
    "\n",
    "model.net_info[\"height\"] = 416\n",
    "inp_dim = int(model.net_info[\"height\"])\n",
    "assert inp_dim % 32 == 0 \n",
    "assert inp_dim > 32\n",
    "\n",
    "#If there's a GPU availible, put the model on GPU\n",
    "\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "\n",
    "model.eval()\n",
    "\n",
    "read_dir = time.time()\n",
    "\n",
    "# Detection phase\n",
    "\n",
    "try:\n",
    "    imlist = [osp.join(osp.realpath('.'), images, img) for img in os.listdir(images)]\n",
    "except NotADirectoryError:\n",
    "    imlist = []\n",
    "    imlist.append(osp.join(osp.realpath('.'), images))\n",
    "except FileNotFoundError:\n",
    "    print (\"No file or directory with the name {}\".format(images))\n",
    "    exit()\n",
    "    \n",
    "if not os.path.exists(\"des\"):\n",
    "    os.makedirs(\"des\")\n",
    "\n",
    "load_batch = time.time()\n",
    "loaded_ims = [cv2.imread(x) for x in imlist]\n",
    "\n",
    "im_batches = list(map(prep_image, loaded_ims, [inp_dim for x in range(len(imlist))]))\n",
    "im_dim_list = [(x.shape[1], x.shape[0]) for x in loaded_ims]\n",
    "im_dim_list = torch.FloatTensor(im_dim_list).repeat(1,2)\n",
    "\n",
    "\n",
    "leftover = 0\n",
    "if (len(im_dim_list) % batch_size):\n",
    "    leftover = 1\n",
    "\n",
    "if batch_size != 1:\n",
    "    num_batches = len(imlist) // batch_size + leftover            \n",
    "    im_batches = [torch.cat((im_batches[i*batch_size : min((i +  1)*batch_size,\n",
    "                        len(im_batches))]))  for i in range(num_batches)]  \n",
    "\n",
    "write = 0\n",
    "\n",
    "if CUDA:\n",
    "    im_dim_list = im_dim_list.cuda()\n",
    "    \n",
    "start_det_loop = time.time()\n",
    "for i, batch in enumerate(im_batches):\n",
    "    # Load the image \n",
    "    start = time.time()\n",
    "    if CUDA:\n",
    "        batch = batch.cuda()\n",
    "    with torch.no_grad():\n",
    "        prediction = model(Variable(batch), CUDA)\n",
    "\n",
    "    prediction = write_results(prediction, confidence, num_classes, nms_conf = nms_thesh)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    if type(prediction) == int:\n",
    "\n",
    "        for im_num, image in enumerate(imlist[i*batch_size: min((i +  1)*batch_size, len(imlist))]):\n",
    "            im_id = i*batch_size + im_num\n",
    "            print(\"{0:20s} predicted in {1:6.3f} seconds\".format(image.split(\"/\")[-1], (end - start)/batch_size))\n",
    "            print(\"{0:20s} {1:s}\".format(\"Objects Detected:\", \"\"))\n",
    "            print(\"----------------------------------------------------------\")\n",
    "        continue\n",
    "\n",
    "    prediction[:,0] += i*batch_size    #transform the atribute from index in batch to index in imlist \n",
    "\n",
    "    if not write:                      #If we have't initialised output\n",
    "        output = prediction  \n",
    "        write = 1\n",
    "    else:\n",
    "        output = torch.cat((output,prediction))\n",
    "\n",
    "    for im_num, image in enumerate(imlist[i*batch_size: min((i +  1)*batch_size, len(imlist))]):\n",
    "        im_id = i*batch_size + im_num\n",
    "        objs = [classes[int(x[-1])] for x in output if int(x[0]) == im_id]\n",
    "        print(\"{0:20s} predicted in {1:6.3f} seconds\".format(image.split(\"/\")[-1], (end - start)/batch_size))\n",
    "        print(\"{0:20s} {1:s}\".format(\"Objects Detected:\", \" \".join(objs)))\n",
    "        print(\"----------------------------------------------------------\")\n",
    "\n",
    "    if CUDA:\n",
    "        torch.cuda.synchronize()       \n",
    "try:\n",
    "    output\n",
    "except NameError:\n",
    "    print (\"No detections were made\")\n",
    "    exit()\n",
    "\n",
    "im_dim_list = torch.index_select(im_dim_list, 0, output[:,0].long())\n",
    "\n",
    "scaling_factor = torch.min(416/im_dim_list,1)[0].view(-1,1)\n",
    "\n",
    "output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim_list[:,0].view(-1,1))/2\n",
    "output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim_list[:,1].view(-1,1))/2\n",
    "\n",
    "output[:,1:5] /= scaling_factor\n",
    "\n",
    "for i in range(output.shape[0]):\n",
    "    output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim_list[i,0])\n",
    "    output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim_list[i,1])\n",
    "    \n",
    "output_recast = time.time()\n",
    "class_load = time.time()\n",
    "colors = [[255, 0, 0], [255, 0, 0], [255, 255, 0], [0, 255, 0], [0, 255, 255], [0, 0, 255], [255, 0, 255]]\n",
    "\n",
    "draw = time.time()\n",
    "\n",
    "def write(x, results):\n",
    "    c1 = tuple(x[1:3].int())\n",
    "    c2 = tuple(x[3:5].int())\n",
    "    img = results[int(x[0])]\n",
    "    cls = int(x[-1])\n",
    "    color = random.choice(colors)\n",
    "    label = \"{0}\".format(classes[cls])\n",
    "    cv2.rectangle(img, c1, c2,color, 1)\n",
    "    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n",
    "    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n",
    "    cv2.rectangle(img, c1, c2,color, -1)\n",
    "    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);\n",
    "    return img\n",
    "\n",
    "\n",
    "list(map(lambda x: write(x, loaded_ims), output))\n",
    "\n",
    "det_names = pd.Series(imlist).apply(lambda x: \"{}/det_{}\".format(\"des\",x.split(\"/\")[-1]))\n",
    "\n",
    "list(map(cv2.imwrite, det_names, loaded_ims))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"SUMMARY\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"{:25s}: {}\".format(\"Task\", \"Time Taken (in seconds)\"))\n",
    "print()\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Reading addresses\", load_batch - read_dir))\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Loading batch\", start_det_loop - load_batch))\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Detection (\" + str(len(imlist)) +  \" images)\", output_recast - start_det_loop))\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Output Processing\", class_load - output_recast))\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Drawing Boxes\", end - draw))\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Average time_per_img\", (end - load_batch)/len(imlist)))\n",
    "print(\"----------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (10 points)\n",
    "\n",
    "In Labs 02-03, you became familiar with different image classification models and the\n",
    "technique of retraining/fine-tuning a pre-trained model on a new dataset. Let's create\n",
    "a ResNet model for classifying images in the CIFAR100 dataset.\n",
    "   \n",
    "First, create dataset objects for the CIFAR100 training and test sets. You'll find\n",
    "documentation at [the torchvision datasets page](https://pytorch.org/vision/stable/datasets.html).\n",
    "To use the already-downloaded dataset on puffer/gourami/guppy, use the following dataset location:\n",
    "\n",
    "    train_dataset = torchvision.datasets.CIFAR100('/home/fidji/mdailey/Datasets/CIFAR100', train=True)\n",
    "\n",
    "Write some code to get one of the samples from the dataset object. Show your code here, and display\n",
    "the image print its attributes here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "# Code to extract a sample from the dataset\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import os\n",
    "from copy import copy\n",
    "from copy import deepcopy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# import os\n",
    "\n",
    "# os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
    "# os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "# Set device to GPU or CPU\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Allow augmentation transform for training set, no augementation for val/test set\n",
    "\n",
    "train_preprocess = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "eval_preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "#full_train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True)\n",
    "\n",
    "full_train_dataset = torchvision.datasets.CIFAR100('/home/fidji/mdailey/Datasets/CIFAR100', download=True)\n",
    "\n",
    "print(len(full_train_dataset))\n",
    "\n",
    "# train_dataset, val_dataset = torch.utils.data.random_split(full_train_dataset, [40000, 10000])\n",
    "# train_dataset.dataset = copy(full_train_dataset)\n",
    "# train_dataset.dataset.transform = train_preprocess\n",
    "# val_dataset.dataset.transform = eval_preprocess\n",
    "\n",
    "#test_dataset = torchvision.datasets.CIFAR100('/home/fidji/mdailey/Datasets/CIFAR100', download=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Show your sample image and its attributes here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (20 points)\n",
    "\n",
    "Next, create data loaders for the training dataset and validation dataset (no need to use the test set).\n",
    "Use a batch size of 4 and appropriate transforms for the training and validation sets.\n",
    "\n",
    "Put your code to create the data loaders, sample one minibatch from the training set, and output the\n",
    "shapes of the tensors comprising the minibatch here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 32, 32])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# Code to create dataloaders, sample a minibatch, and print out tensor shape here\n",
    "\n",
    "# DataLoaders for the three datasets\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_train_dataset, [40000, 10000])\n",
    "train_dataset.dataset = copy(full_train_dataset)\n",
    "train_dataset.dataset.transform = train_preprocess\n",
    "val_dataset.dataset.transform = eval_preprocess\n",
    "\n",
    "BATCH_SIZE=4\n",
    "NUM_WORKERS=4\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                                            shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
    "                                            shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "\n",
    "dataloaders = {'train': train_dataloader, 'val': val_dataloader}\n",
    "\n",
    "\n",
    "\n",
    "for inputs, labels in train_dataloader:\n",
    "\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    print(inputs.shape)\n",
    "    print(labels.shape)\n",
    "    \n",
    "\n",
    "    # outputs = resnet(inputs)\n",
    "    # print('outputs', outputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (20 points)\n",
    "\n",
    "Next, create a ResNet-50 model with pretrained weights from ImageNet using the [torchvision ResNet class](https://pytorch.org/vision/stable/models.html#id10).\n",
    "Remove the classification layer and replace it with a layer appropriate for identification in CIFAR100. Show that your resulting model can process a minibatch\n",
    "from your validation dataloader and output (incorrect) identities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to create a ResNet 50 model, remove classification layer, replace with a CIFAR100 identity layer, and run in evaluation model on a validation minibatch\n",
    "class BasicBlock(nn.Module):\n",
    "    '''\n",
    "    BasicBlock: Simple residual block with two conv layers\n",
    "    '''\n",
    "    EXPANSION = 1\n",
    "    def __init__(self, in_planes, out_planes, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_planes)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        # If output size is not equal to input size, reshape it with 1x1 convolution\n",
    "        if stride != 1 or in_planes != out_planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckBlock(nn.Module):\n",
    "    '''\n",
    "    BottleneckBlock: More powerful residual block with three convs, used for Resnet50 and up\n",
    "    '''\n",
    "    EXPANSION = 4\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.EXPANSION * planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.EXPANSION * planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        # If the output size is not equal to input size, reshape it with 1x1 convolution\n",
    "        if stride != 1 or in_planes != self.EXPANSION * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.EXPANSION * planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.EXPANSION * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.in_planes = 64\n",
    "        # Initial convolution\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        # Residual blocks\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        # FC layer = 1 layer\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.linear = nn.Linear(512 * block.EXPANSION, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.EXPANSION\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50(num_classes = 100):\n",
    "    '''\n",
    "    First conv layer: 1\n",
    "    4 residual blocks with [3, 4, 6, 3] sets of three convolutions each: 3*3 + 4*3 + 6*3 + 3*3 = 48\n",
    "    last FC layer: 1\n",
    "    Total layers: 1+48+1 = 50\n",
    "    '''\n",
    "    return ResNet(BottleneckBlock, [3, 4, 6, 3], num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n",
      "number of trainable parameters: 23705252\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BottleneckBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BottleneckBlock(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): BottleneckBlock(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BottleneckBlock(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BottleneckBlock(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): BottleneckBlock(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (3): BottleneckBlock(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BottleneckBlock(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BottleneckBlock(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): BottleneckBlock(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (3): BottleneckBlock(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (4): BottleneckBlock(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (5): BottleneckBlock(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BottleneckBlock(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BottleneckBlock(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): BottleneckBlock(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (linear): Linear(in_features=2048, out_features=100, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using device', device)\n",
    "\n",
    "resnet = ResNet50().to(device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'number of trainable parameters: {count_parameters(resnet)}')\n",
    "\n",
    "print(resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs tensor([[ 8.9998e-01,  1.6702e+00, -1.0239e-01,  3.9116e-01,  3.2572e-01,\n",
      "         -4.0929e-01,  1.5373e+00,  1.3315e-01, -4.5107e-02,  1.8990e-01,\n",
      "         -8.3797e-01,  1.1464e+00,  6.7381e-01,  1.1757e-01,  1.2707e+00,\n",
      "         -1.0679e+00,  1.2887e-01, -3.1658e-01,  6.1735e-01, -6.1088e-01,\n",
      "         -1.1022e-01,  8.3536e-01, -3.2672e-01,  1.3450e+00,  6.0671e-01,\n",
      "         -8.8988e-01,  9.9034e-01, -8.2813e-01,  2.5868e-01,  1.9961e-01,\n",
      "         -2.8454e-01,  4.8063e-02,  4.4529e-01,  9.3352e-01,  1.9787e+00,\n",
      "          4.3030e-01, -5.2796e-01,  9.5990e-01, -2.3635e+00,  1.2462e+00,\n",
      "          5.8215e-01, -3.0574e-01, -5.4765e-01, -1.2662e+00,  1.4668e+00,\n",
      "         -7.2958e-01, -2.5948e-01,  7.2205e-01, -1.1518e+00, -3.7021e-01,\n",
      "          6.0187e-01,  1.9724e-01, -6.2571e-01,  8.3927e-01, -9.8660e-01,\n",
      "         -5.7788e-01,  4.1529e-01,  2.9495e-01, -2.9577e-02, -1.2082e+00,\n",
      "         -1.1462e+00, -8.8534e-01, -2.2149e-01,  1.5619e-01,  8.9053e-01,\n",
      "         -5.3656e-02, -1.1022e+00,  5.6012e-01, -6.9934e-01,  4.9124e-01,\n",
      "         -8.7174e-01, -3.1645e-01, -1.0918e+00, -9.2082e-01, -4.0551e-01,\n",
      "         -9.1574e-01, -3.5136e-01,  4.2576e-01, -5.9874e-01, -2.9588e-01,\n",
      "         -1.2578e-01,  4.7166e-01,  1.5194e-01,  1.2196e+00, -1.2163e-01,\n",
      "         -6.4394e-01,  4.3781e-01,  8.9811e-01, -3.9048e-01,  4.6303e-02,\n",
      "          9.5334e-02, -7.2972e-01, -1.0542e+00,  3.6290e-01,  6.0306e-01,\n",
      "         -4.1401e-02,  1.6764e-01, -9.0179e-01,  7.9255e-01,  2.4306e-01],\n",
      "        [ 3.3652e-01,  9.7995e-01, -7.6092e-02,  2.4922e-01,  6.5185e-02,\n",
      "         -4.2356e-01,  7.5814e-01,  2.1694e-01,  4.0049e-02,  3.6240e-01,\n",
      "         -3.2533e-01,  1.2483e+00,  4.3076e-01,  4.2888e-01,  6.1150e-01,\n",
      "         -1.1189e+00,  7.7292e-04, -1.7045e-01,  1.7668e-01, -3.0646e-01,\n",
      "         -2.0876e-01,  1.0809e+00, -2.8076e-01,  1.1685e+00,  5.5551e-01,\n",
      "         -6.7568e-01,  8.9358e-01, -6.6817e-01,  5.4225e-01, -2.7501e-01,\n",
      "         -7.1430e-01,  2.4459e-01,  2.7625e-01,  7.2105e-01,  1.1253e+00,\n",
      "          1.2990e-01, -9.9642e-01,  8.0834e-01, -1.0307e+00,  9.6251e-01,\n",
      "         -1.4622e-01,  4.9757e-02, -4.6263e-01, -5.3749e-01,  7.2816e-01,\n",
      "         -1.0260e-01, -8.9366e-03,  5.6065e-01, -2.6952e-01, -2.7664e-01,\n",
      "          2.6980e-01,  7.1925e-01, -6.5756e-01,  2.8432e-01, -1.1414e+00,\n",
      "         -7.7135e-01,  2.1849e-01,  1.3906e-01, -3.1426e-01, -8.0672e-01,\n",
      "         -4.1695e-01, -7.6113e-01, -6.5145e-02, -6.4082e-02,  5.7508e-01,\n",
      "          2.0735e-01, -3.9553e-01,  5.7266e-01, -4.6811e-01,  7.7756e-01,\n",
      "         -1.7605e-01,  2.5759e-02, -4.5971e-01, -1.0344e+00,  1.4466e-01,\n",
      "         -8.2333e-01, -2.0018e-01,  5.7777e-01, -1.4065e-01, -1.6009e-01,\n",
      "         -5.3490e-01,  2.3186e-01, -2.0280e-05,  3.2391e-01, -5.1936e-01,\n",
      "         -4.8170e-01,  2.7292e-01,  5.3359e-01, -5.8469e-02,  3.4338e-01,\n",
      "          1.1075e-01, -3.7595e-01, -6.7051e-01, -6.2576e-02,  4.0793e-01,\n",
      "          7.9966e-03, -1.9656e-01, -4.7401e-01,  1.4209e-01, -2.8892e-01],\n",
      "        [ 4.6785e-01,  1.1490e+00, -5.5182e-01,  3.3921e-01,  4.9347e-02,\n",
      "         -4.4134e-01,  8.6528e-01,  2.0787e-01,  2.5063e-01,  5.2560e-01,\n",
      "         -3.5657e-01,  1.2278e+00,  4.1537e-01,  6.9597e-01,  1.0183e+00,\n",
      "         -1.1772e+00, -2.4501e-01, -1.0089e-01,  2.2724e-01, -1.2840e-01,\n",
      "         -1.1995e-01,  9.2416e-01, -1.2651e-01,  1.1671e+00,  2.7198e-01,\n",
      "         -6.4583e-01,  8.3780e-01, -9.0966e-01,  4.0444e-01,  9.9777e-02,\n",
      "         -3.1198e-01,  4.0287e-01,  1.8299e-01,  6.7662e-01,  1.4482e+00,\n",
      "          2.6842e-01, -9.1646e-01,  7.3840e-01, -1.3286e+00,  7.9004e-01,\n",
      "          5.3317e-03,  1.1500e-02, -6.6670e-01, -8.5721e-01,  7.7029e-01,\n",
      "         -4.7755e-01,  1.0883e-01,  5.8415e-01, -2.9199e-01, -1.3133e-01,\n",
      "          5.3171e-01,  5.3652e-01, -1.0282e+00,  3.2304e-01, -1.0194e+00,\n",
      "         -5.1697e-01,  3.9382e-01,  4.0261e-02, -3.9823e-02, -8.5380e-01,\n",
      "         -7.6522e-01, -8.1315e-01, -1.5117e-01, -1.3390e-01,  8.3196e-01,\n",
      "          7.7147e-02, -3.4003e-01,  5.4326e-01, -5.6658e-01,  7.2667e-01,\n",
      "         -7.3736e-01, -6.7738e-02, -6.7341e-01, -7.0380e-01,  3.3970e-01,\n",
      "         -7.0079e-01, -6.6631e-01,  4.7850e-01, -2.0972e-01, -4.2343e-01,\n",
      "         -3.7236e-01,  1.1773e-01,  1.0293e-02,  8.8695e-01, -2.1974e-01,\n",
      "         -6.4167e-01,  2.8313e-01,  3.4416e-01, -1.5859e-01,  1.0596e-01,\n",
      "          3.0805e-01, -6.7648e-01, -9.6337e-01,  4.2950e-01,  5.7700e-01,\n",
      "         -1.4313e-01, -4.3527e-01, -6.4001e-01,  3.5543e-01,  5.4640e-02],\n",
      "        [ 1.1878e-01,  1.2880e+00, -1.8852e-01,  3.4545e-01,  4.0316e-01,\n",
      "         -4.8628e-01,  9.1431e-01, -2.1486e-01, -5.8963e-02,  9.1988e-02,\n",
      "         -3.3946e-01,  1.3948e+00,  3.4573e-01,  6.1785e-01,  1.1081e+00,\n",
      "         -1.5921e+00, -2.6362e-01, -2.7745e-01,  4.1687e-01, -3.2295e-01,\n",
      "         -1.6892e-01,  9.9446e-01, -2.4276e-01,  1.3043e+00,  4.6045e-01,\n",
      "         -1.0582e+00,  7.6477e-01, -8.1304e-01,  4.5012e-01, -2.1552e-01,\n",
      "         -6.2382e-01,  3.3891e-01,  5.7551e-01,  8.1190e-01,  1.6309e+00,\n",
      "          6.8185e-01, -9.9550e-01,  1.0205e+00, -1.6347e+00,  1.2368e+00,\n",
      "          4.4837e-01,  6.8465e-02, -6.3306e-01, -1.0017e+00,  1.0282e+00,\n",
      "         -3.7839e-01,  3.5995e-02,  9.0508e-01, -7.3571e-01, -3.2504e-01,\n",
      "          3.2560e-01,  4.3751e-01, -6.8331e-01,  5.8928e-01, -1.0285e+00,\n",
      "         -5.4440e-01, -1.3308e-02,  3.1297e-01, -4.2229e-01, -1.0400e+00,\n",
      "         -5.7200e-01, -9.1167e-01,  1.4999e-01,  1.1832e-01,  1.7518e-01,\n",
      "          3.9479e-01, -6.4971e-01,  4.6504e-01, -2.2483e-01,  6.6389e-01,\n",
      "         -9.9112e-01, -1.6883e-01, -8.0644e-01, -8.9429e-01, -7.7068e-02,\n",
      "         -9.3185e-01, -4.1487e-01,  5.4179e-01, -3.2033e-01, -5.3375e-02,\n",
      "         -1.6528e-01,  5.9681e-01,  2.9799e-01,  9.8440e-01, -4.4777e-01,\n",
      "         -7.0076e-01,  5.3666e-01,  6.3056e-01, -1.0645e-01,  4.2828e-01,\n",
      "          7.5125e-01, -6.5072e-01, -1.1454e+00,  5.4178e-01,  2.4802e-01,\n",
      "         -5.6690e-02, -2.1093e-01, -7.8539e-01,  6.7639e-01, -6.5312e-02]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "torch.Size([4, 100])\n"
     ]
    }
   ],
   "source": [
    "for inputs, labels in val_dataloader:\n",
    "\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    outputs = resnet(inputs)\n",
    "    print('outputs', outputs)\n",
    "    print(outputs.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 (20 points)\n",
    "\n",
    "Next, write a training function, create an optimizer and loss function, and show training loss and validation loss for one epoch.\n",
    "\n",
    "Show the new ouptut identities for the validation minibatch used in Question 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, weights_name='weight_save', is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    loss_acc_history = []\n",
    "\n",
    "    best_model_wts = deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  \n",
    "            else:\n",
    "                model.eval()  \n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "           \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "             \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                   \n",
    "                    if is_inception and phase == 'train':\n",
    "                      \n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                       \n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            epoch_end = time.time()\n",
    "            \n",
    "            elapsed_epoch = epoch_end - epoch_start\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            print(\"Epoch time taken: \", elapsed_epoch)\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), weights_name + \".pth\")\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "            if phase == 'train':\n",
    "                loss_acc_history.append(epoch_loss)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history, loss_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n",
      "train Loss: 2.0762 Acc: 0.2142\n",
      "Epoch time taken:  616.2770862579346\n",
      "val Loss: 2.2004 Acc: 0.2828\n",
      "Epoch time taken:  665.5300993919373\n",
      "\n",
      "Training complete in 11m 7s\n",
      "Best val Acc: 0.282800\n"
     ]
    }
   ],
   "source": [
    "# Code for training, optimizer, loss here, training one epoch, and new result on validation minibatch\n",
    "# Optimizer and loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "params_to_update = resnet.parameters()\n",
    "# Now we'll use Adam optimization\n",
    "optimizer = optim.Adam(params_to_update, lr=0.01)\n",
    "\n",
    "best_model, val_acc_history, loss_acc_history = train_model(resnet, dataloaders, criterion, optimizer, 1, 'resnet18_bestsofar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 (10 points)\n",
    "\n",
    "Explain how you could use the model you just created as a classifier model in a Control GAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Put your explanation here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several studies have been conducted for using a classifier to address the problem. Auxiliary Classifier GAN (AC-GAN)uses a classifier as the discriminator of GAN structure. Triple-GAN uses the classification results as an input for discriminator. However, such methods commonly use a classifier that is attached to a discriminator.\n",
    "\n",
    "ControlGAN is composed of three neural network structures, which are a generator/decoder, a discriminator and a classifier/encoder. Three- player game is conducted in ControlGAN where the generator tries to deceive the discriminator, which is the same as vanilla GAN, and simultaneously aim to be classified corresponding class by the classifier. The generator and the classifier can be interpreted as a decoder-encoder structure because labels are commonly used for inputs for the generator and outputs for the classifier.\n",
    "\n",
    "\n",
    "θD =argmin{α·LD(tD,D(x;θD))+(1−α)·LD((1−tD),D(G(z,l;θG);θD))}, \n",
    "θG = argmin{γt ·LC(l,G(z,l;θG))+LD(tD,D(G(z,l;θG);θD))}, \n",
    "θC =argmin{LC(l,x;θC)},\n",
    "\n",
    "\n",
    "\n",
    "where l is the binary representation of labels of sample x and input data for the generator, tD is the label for discriminator which we set to one in this work, and α denotes a parameter for the discriminator.\n",
    "ControlGAN forces features to be mapped onto corresponding l inputted into the generator. The parameter γt decides how much the generator focus on the input labels for the generator.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
