{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23a9f4da-00ec-439e-9872-a6abc4e927cb",
   "metadata": {},
   "source": [
    "Introduction: the background and goals of the lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a1262e-4789-43b8-be90-8168fd570e9f",
   "metadata": {},
   "source": [
    "Methods: what you did, what parameters you tried, and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129063f8-f766-4e43-9d7c-ca63ae772488",
   "metadata": {},
   "source": [
    "Results: what were the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60983011-dec0-414f-951c-85f882cf6aab",
   "metadata": {},
   "source": [
    "Conclusion: what did you learn from the lab, and what might be the next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7340ba1-64ee-4604-ba74-73f53bd71ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import os\n",
    "os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e5cf098-6d4b-416f-9d2b-5e4ca5fc3d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/st122314/.cache/torch/hub/pytorch_vision_v0.5.0\n"
     ]
    }
   ],
   "source": [
    "#Download pretrained Alexnext Model\n",
    "model = torch.hub.load('pytorch/vision:v0.5.0', 'alexnet', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9299b5f-2af0-4172-8043-ce8f519ed5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd98af2d-14c3-48ec-a83a-b00602cdb4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.1 Modify the output of AlexNet to predict 10 classes\n",
    "model.classifier[1] = nn.Linear(9216,4096)\n",
    "model.classifier[4] = nn.Linear(4096,1024)\n",
    "model.classifier[6] = nn.Linear(1024,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc86af49-c5d1-46a4-bd87-e83e07ce48dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# after changing the output layer, let's see the modified architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "265643a1-51a1-403c-823a-9d2dbde188d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b81cb91-a7de-481f-81be-960c004243fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c35f8fd1-0638-4da0-8ef6-e6fea006acaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_val_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4a06151-a9b2-46f5-8023-edefa27136f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.datasets.cifar.CIFAR10'>\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(type(train_val_data))\n",
    "print(len(train_val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c909cc18-ee81-4386-af69-72bd704dc700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and Test Data split\n",
    "#Train test split\n",
    "train_size = int(len(train_val_data) * 0.7)\n",
    "valid_size = int(len(train_val_data) * 0.3)\n",
    "\n",
    "train_set, valid_set = torch.utils.data.random_split(train_val_data, [train_size, valid_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b362d304-c4d5-4235-83eb-c3f3a404af19",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 #keeping it binary so it fits GPU\n",
    "\n",
    "#Train set loader\n",
    "train_iterator = torch.utils.data.DataLoader(dataset=train_set, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           shuffle=True, pin_memory = True)\n",
    "#Validation set loader\n",
    "valid_iterator = torch.utils.data.DataLoader(dataset=valid_set, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           shuffle=True, pin_memory = True)\n",
    "\n",
    "#Test set loader\n",
    "test_iterator = torch.utils.data.DataLoader(dataset=test_data, \n",
    "                                          batch_size=BATCH_SIZE, \n",
    "                                          shuffle=True, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a735bd30-619b-448e-ae1b-cce3336eb772",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Train and Evaluate function\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    predicteds = []\n",
    "    trues = []    \n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch, labels in iterator:\n",
    "\n",
    "        #Move tensors to the configured device\n",
    "        batch = batch.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #Forward pass\n",
    "        outputs = model(batch.float())\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "        #check accuracy\n",
    "        predictions = model(batch.float()) #<<< already updated\n",
    "        predictions = nn.functional.softmax(predictions, dim=1)\n",
    "        _, predicted = torch.max(predictions.data, 1)  #returns max value, indices\n",
    "        predicteds.append(predicted)\n",
    "        trues.append(labels)        \n",
    "        total += labels.size(0)  #keep track of total\n",
    "        correct += (predicted == labels).sum().item()  #.item() give the raw number\n",
    "        acc = 100 * (correct / total)\n",
    "                \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc = acc\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc,predicteds, trues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c908e7e-a21d-4b92-ab61-1ce47bb1aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    predicteds = []\n",
    "    trues = []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch, labels in iterator:\n",
    "            \n",
    "            #Move tensors to the configured device\n",
    "            batch = batch.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            predictions = model(batch.float())   \n",
    "            loss = criterion(predictions, labels.long())\n",
    "            \n",
    "            predictions = nn.functional.softmax(predictions, dim=1)            \n",
    "            _, predicted = torch.max(predictions.data, 1)  #returns max value, indices\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "#             print('================== Predicted y ====================')\n",
    "#             print(predicted) \n",
    "#             print('==================    True y   ====================')\n",
    "#             print(labels)            \n",
    "            predicteds.append(predicted)\n",
    "            trues.append(labels)            \n",
    "            total += labels.size(0)  #keep track of total\n",
    "            correct += (predicted == labels).sum().item()  #.item() give the raw number\n",
    "            acc = 100 * (correct / total)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator),predicteds, trues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22eeb51f-a15a-43af-96ca-9a07c59a8a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model AlexNet has 44,428,106 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "#Count the parameter\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# for model in models:\n",
    "print(f'The model {type(model).__name__} has {count_parameters(model):,} trainable parameters')# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c735903b-50c5-48d6-bd71-7fe39b04b0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Loss and optimizer function\n",
    "learning_rate = 0.01\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "145cc19c-9073-4b61-924a-da71e7dd182c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured device:  cuda\n"
     ]
    }
   ],
   "source": [
    "#from chosen_gpu import get_freer_gpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "print(\"Configured device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd315d65-9a5a-436e-944b-4f5d48b5c65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "# model.load_state_dict(torch.load('checkpointss/alexnet-cifar-10-%02d-epochs-sgd-0.01.pth' % epoch))\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "409eec8a-cb52-4419-966a-174338537019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537b7078-8d29-4b1f-98cc-57fab11c87b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 0\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "N_EPOCHS = 15\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "train_accs = []\n",
    "valid_accs = []\n",
    "\n",
    "# train_predicted_labels = []\n",
    "# valid_predicted_labels = []\n",
    "\n",
    "# train_true_labels = []\n",
    "# valid_true_labels = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f'training {epoch}')\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc, _, _ = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc, _, _ = evaluate(model, valid_iterator, criterion)\n",
    "    train_losses.append(train_loss); train_accs.append(train_acc);  \n",
    "    valid_losses.append(valid_loss); valid_accs.append(valid_acc);\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)  \n",
    "    \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        clear_output(wait=True)            \n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc:.2f}%')\n",
    "\n",
    "#     if valid_loss < best_valid_loss:\n",
    "#         best_valid_loss = valid_loss\n",
    "#         print(\"Model:{} saved.\".format(type(model).__name__))\n",
    "# #         torch.save(model.state_dict(), f'../notebooks_beau/{type(model).__name__}{i}.pth.tar')\n",
    "#         torch.save(model.state_dict(), ('checkpoints_lab01/alexnet-cifar-10-%02d-epochs-sgd-0.01.pth' % epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5b6356-a723-4d2a-8584-26450eb3a1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Result\n",
    "#The model at the epoch 10 performed the best with the accuracy of 88.18%.\n",
    "# model.load_state_dict(torch.load('checkpoints/alexnet-cifar-10-%02d-epochs-sgd-0.01.pth' % 10))\n",
    "\n",
    "test_loss, test_acc, test_pred_label, test_true_label  = evaluate(model, test_iterator, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad5ba2e-cd5b-421e-b0b7-2e48cb7ec567",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c21a73-06e5-487e-a0c9-d24d03d95d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_pred_label[-1])\n",
    "print(test_true_label[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea44d3c3-8650-483b-9806-b39b1780129b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,1,sharex=True,figsize=(10,10))\n",
    "ax[0].plot(np.arange(N_EPOCHS),train_losses,label = \"train loss\")\n",
    "ax[0].plot(np.arange(N_EPOCHS),valid_losses, label = \"valid loss\")\n",
    "ax[1].plot(np.arange(N_EPOCHS),train_accs,label = \"train acc\")\n",
    "ax[1].plot(np.arange(N_EPOCHS),valid_accs,label = \"valid acc\")\n",
    "\n",
    "plt.subplots_adjust(hspace=0.03)\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[0].grid(True)\n",
    "ax[1].grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c84dbb-d559-4d95-b5ed-606fbbc83b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
