{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled1.ipynb","provenance":[],"authorship_tag":"ABX9TyPySWUVYuEYWAWAlHE/A0z/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"VM44SkUuhguX","executionInfo":{"status":"ok","timestamp":1651927055558,"user_tz":-420,"elapsed":853,"user":{"displayName":"Win Win Phyo","userId":"12747411063281597081"}}},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n","from copy import copy\n","from six import StringIO\n","from pprint import pprint\n","\n","import gym\n","from gym import spaces, error, utils\n","from gym.utils import seeding\n","import numpy as np\n","\n","\n","resolved = 0\n","unfinished = 1\n","error = 2\n","\n","\n","def checkSolution(grid):\n","    N = len(grid)\n","    for i in range(N):\n","        for j in range(N):\n","            if grid[i][j] == 0:\n","                return unfinished\n","            n = N // 3\n","            iOffset = i // n * n\n","            jOffset = j // n * n\n","            square = grid[iOffset:iOffset + n, jOffset:jOffset + n].flatten()\n","\n","            uniqueInRow = countItem(grid[i], grid[i, j]) == 1\n","            uniqueInCol = countItem(grid[:, j:j + 1].flatten(), grid[i, j]) == 1\n","            uniqueInSquare = countItem(square, grid[i, j]) == 1\n","\n","            if not (uniqueInRow and uniqueInCol and uniqueInSquare):\n","                return error\n","    return resolved\n","\n","\n","def countItem(vector, item):\n","    count = 0\n","    for item2 in vector:\n","        if item2 == item: count += 1\n","    return count\n","\n","\n","def getSolutions(grid, stopAt=1, i=-1, j=-1, omit=-1):\n","    N = len(grid)\n","    check = checkSolution(grid)\n","\n","    if check == resolved:\n","        return np.array([grid], dtype=int)\n","    if check == error:\n","        return np.empty(shape=(0, N, N), dtype=int)\n","\n","    if i == -1:\n","        for i in range(N):\n","            for j in range(N):\n","                if grid[i, j] == 0: break\n","            if grid[i, j] == 0: break\n","\n","    values = np.arange(1, N + 1)\n","    np.random.shuffle(values)\n","\n","    solutions = np.empty(shape=(0, N, N), dtype=int)\n","    for value in values:\n","        if omit == value: continue\n","        cGrid = np.copy(grid)\n","        cGrid[i, j] = value\n","        subSolutions = getSolutions(cGrid, stopAt=stopAt - len(solutions))\n","        solutions = np.concatenate((solutions, subSolutions))\n","        if len(solutions) >= stopAt:\n","            return solutions\n","    return solutions\n","\n","\n","class SudokuEnv(gym.Env):\n","    last_action = None\n","\n","    def __init__(self, n):\n","        self.n = n\n","        self.val_limit = self.n - 1\n","        self.nxn = self.n * self.n\n","        self.observation_space = spaces.Box(1, self.n, shape=(self.n, self.n))\n","        # Action = nxn*value + (row*n + col), n = 9 here\n","        self.action_space = spaces.Discrete(self.nxn * self.val_limit + (self.val_limit * self.n + self.val_limit))\n","        self.grid = []\n","        self.original_indices_row = []\n","        self.original_indices_col = []\n","        self.base = getSolutions(np.zeros(shape=(self.n, self.n)))[0]\n","\n","        N = len(self.base)\n","        positions = []\n","        for i in range(N):\n","            for j in range(N):\n","                positions.append((i, j))\n","        np.random.shuffle(positions)\n","\n","        count = 0\n","        for i, j in positions:\n","            if count > 1:\n","                break\n","            oldValue = self.base[i, j]\n","            self.base[i, j] = 0\n","            solutions = getSolutions(self.base, stopAt=2, i=i, j=j, omit=oldValue)\n","            if len(solutions) == 0:\n","                count += 1\n","            else:\n","                self.base[i, j] = oldValue\n","\n","    def step(self, action):\n","        \"\"\"\n","        \"\"\"\n","        if self.last_action != None and self.last_action == action:\n","            return np.copy(self.grid), -0.5, False, None\n","        self.last_action = action\n","        oldGrid = np.copy(self.grid)\n","\n","        square = action % self.nxn\n","        col = square % self.n\n","        row = (square - col) // self.n\n","        val = action // self.nxn + 1\n","\n","        for i in range(len(self.original_indices_row)):\n","            if col == self.original_indices_col[i] and row == self.original_indices_row[i]:\n","                # print(f\"ORIGINAL FILL Row: {row} Column: {col} Value: {val}\")\n","                return np.copy(self.grid), -1, False, None\n","\n","        if self.grid[row, col] == val:\n","            # print(\"Already there\")\n","            return np.copy(self.grid), -1, False, None\n","\n","        self.grid[row, col] = val\n","\n","        stats = checkSolution(self.grid)\n","        if stats == resolved:\n","            return np.copy(self.grid), 1, True, None\n","        elif stats == unfinished:\n","            return np.copy(self.grid), -0.1, False, None\n","        elif stats == error:\n","            self.grid = oldGrid\n","            return np.copy(self.grid), -1, False, None\n","\n","    def reset(self):\n","        self.last_action = None\n","        self.grid = np.copy(self.base)\n","        self.original_indices_row, self.original_indices_col = np.nonzero(self.grid)\n","        return np.copy(self.grid)\n","\n","    def render(self, mode='human', close=False):\n","        if self.last_action != None:\n","            square = self.last_action % self.nxn\n","            col = square % self.n\n","            row = (square - col) // self.n\n","            val = self.last_action // self.nxn\n","        for i in range(len(self.grid)):\n","            for j in range(len(self.grid)):\n","                if self.last_action != None and i == row and j == col:\n","                    if val == self.grid[i, j]:\n","                        print('')\n","                    else:\n","                        print('')\n","                else:\n","                    print('')\n","                if j % 3 == 2 and j != len(self.grid) - 1:\n","                    print(' | ')\n","            if i % 3 == 2 and i != len(self.grid) - 1:\n","                print('\\n-------------------------\\n')\n","            else:\n","                print('\\n')\n","        print('\\n\\n')"]},{"cell_type":"code","source":["env= SudokuEnv(9)\n"],"metadata":{"id":"g5bGwXpChp7L","executionInfo":{"status":"ok","timestamp":1651927470472,"user_tz":-420,"elapsed":417,"user":{"displayName":"Win Win Phyo","userId":"12747411063281597081"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import math, random\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.autograd as autograd \n","import torch.nn.functional as F\n","\n","import matplotlib.pyplot as plt\n","\n","import gym\n","import numpy as np\n","\n","from collections import deque\n","from tqdm import trange\n","\n","# Select GPU or CPU as device\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"ZduSyLouhwpM","executionInfo":{"status":"ok","timestamp":1651927368428,"user_tz":-420,"elapsed":3384,"user":{"displayName":"Win Win Phyo","userId":"12747411063281597081"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# epsilon_start = 1.0\n","# epsilon_final = 0.01\n","# epsilon_decay = 500\n","\n","# # Define epsilon as a function of time (episode index)\n","\n","# eps_by_episode = lambda episode: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * episode / epsilon_decay)\n","\n","# # Note that the above lambda expression is equivalent to explicitly defining a function:\n","# # def epsilon_episode(episode):\n","# #     return epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * episode / epsilon_decay)\n","\n","# plt.plot([eps_by_episode(i) for i in range(10000)])\n","# plt.title('Epsilon as function of time')\n","# plt.xlabel('Time (episode index)')\n","# plt.ylabel('Epsilon')\n","# plt.show()"],"metadata":{"id":"MJaveZyLiy7w","executionInfo":{"status":"ok","timestamp":1651927942207,"user_tz":-420,"elapsed":544,"user":{"displayName":"Win Win Phyo","userId":"12747411063281597081"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# Epsilon annealing schedule generator\n","\n","def gen_eps_by_episode(epsilon_start, epsilon_final, epsilon_decay):\n","    eps_by_episode = lambda episode: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * episode / epsilon_decay)\n","    return eps_by_episode\n","\n","epsilon_start = 1.0\n","epsilon_final = 0.01\n","epsilon_decay = 500\n","eps_by_episode = gen_eps_by_episode(epsilon_start, epsilon_final, epsilon_decay)"],"metadata":{"id":"sFdWuedciy-2","executionInfo":{"status":"ok","timestamp":1651927403281,"user_tz":-420,"elapsed":515,"user":{"displayName":"Win Win Phyo","userId":"12747411063281597081"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["class ReplayBuffer(object):\n","    def __init__(self, capacity):\n","        self.buffer = deque(maxlen=capacity)\n","    \n","    def push(self, state, action, reward, next_state, done):\n","        # Add batch index dimension to state representations\n","        state = np.expand_dims(state, 0)\n","        next_state = np.expand_dims(next_state, 0)            \n","        self.buffer.append((state, action, reward, next_state, done))\n","    \n","    def sample(self, batch_size):\n","        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n","        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n","    \n","    def __len__(self):\n","        return len(self.buffer)"],"metadata":{"id":"wpCOs6RCizBj","executionInfo":{"status":"ok","timestamp":1651927415815,"user_tz":-420,"elapsed":2,"user":{"displayName":"Win Win Phyo","userId":"12747411063281597081"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class DQN(nn.Module):\n","    \n","    def __init__(self, n_state, n_action):\n","        super(DQN, self).__init__()        \n","        self.layers = nn.Sequential(\n","            nn.Linear(n_state, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, n_action)\n","        )\n","        \n","    def forward(self, x):\n","        return self.layers(x)\n","    \n","    def act(self, state, epsilon):\n","        # Get an epsilon greedy action for given state\n","        if random.random() > epsilon: # Use argmax_a Q(s,a)\n","            state = autograd.Variable(torch.FloatTensor(state).unsqueeze(0), volatile=True).to(device)\n","            q_value = self.forward(state)\n","            q_value = q_value.cpu().numpy()\n","            action = q_value.max(1)[1].item()         \n","        else: # get random action\n","            action = random.randrange(env.action_space.n)\n","        return action"],"metadata":{"id":"TWr44gBpjDER","executionInfo":{"status":"ok","timestamp":1651927824402,"user_tz":-420,"elapsed":434,"user":{"displayName":"Win Win Phyo","userId":"12747411063281597081"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["env.observation_space.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xEi_s2yWnmTj","executionInfo":{"status":"ok","timestamp":1651928633189,"user_tz":-420,"elapsed":427,"user":{"displayName":"Win Win Phyo","userId":"12747411063281597081"}},"outputId":"54327116-cbef-45be-84f3-06e2fa930961"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(9, 9)"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["env.action_space.n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MAIMf4-KoJUB","executionInfo":{"status":"ok","timestamp":1651928768404,"user_tz":-420,"elapsed":8,"user":{"displayName":"Win Win Phyo","userId":"12747411063281597081"}},"outputId":"80249f5f-8ce9-4536-d2ae-7eec4c0ccfdb"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["728"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["model = DQN(env.observation_space.shape[0], env.action_space.n).to(device)\n","    \n","optimizer = optim.Adam(model.parameters())\n","\n","replay_buffer = ReplayBuffer(1000)"],"metadata":{"id":"khwWuZQGjDG1","executionInfo":{"status":"ok","timestamp":1651927474408,"user_tz":-420,"elapsed":409,"user":{"displayName":"Win Win Phyo","userId":"12747411063281597081"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def compute_td_loss(model, batch_size, gamma=0.99):\n","\n","    # Get batch from replay buffer\n","    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n","\n","    # Convert to tensors. Creating Variables is not necessary with more recent PyTorch versions.\n","    state      = autograd.Variable(torch.FloatTensor(np.float32(state))).to(device)\n","    next_state = autograd.Variable(torch.FloatTensor(np.float32(next_state)), volatile=True).to(device)\n","    action     = autograd.Variable(torch.LongTensor(action)).to(device)\n","    reward     = autograd.Variable(torch.FloatTensor(reward)).to(device)\n","    done       = autograd.Variable(torch.FloatTensor(done)).to(device)\n","\n","    # Calculate Q(s) and Q(s')\n","    q_values      = model(state)\n","    next_q_values = model(next_state)\n","\n","    print(\"q_values.shape\", q_values.shape)\n","    print(\"action.shape\",action.shape)\n","\n","    # Get Q(s,a) and max_a' Q(s',a')\n","    action_ = action.unsqueeze(1)\n","    print(\"action_.shape\", action_.shape)\n","    q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n","    #q_value          = q_values.gather(1, action_.unsqueeze(1))\n","    next_q_value     = next_q_values.max(1)[0]\n","    # Calculate target for Q(s,a): r + gamma max_a' Q(s',a')\n","    # Note that the done signal is used to terminate recursion at end of episode.\n","    expected_q_value = reward + gamma * next_q_value * (1 - done)\n","    \n","    # Calculate MSE loss. Variables are not needed in recent PyTorch versions.\n","    loss = (q_value - autograd.Variable(expected_q_value.data)).pow(2).mean()\n","        \n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    return loss"],"metadata":{"id":"ZiReMfy5jDJD","executionInfo":{"status":"ok","timestamp":1651928920472,"user_tz":-420,"elapsed":406,"user":{"displayName":"Win Win Phyo","userId":"12747411063281597081"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["def plot(episode, rewards, losses):\n","    # clear_output(True)\n","    plt.figure(figsize=(20,5))\n","    plt.subplot(131)\n","    plt.title('episode %s. reward: %s' % (episode, np.mean(rewards[-10:])))\n","    plt.plot(rewards)\n","    plt.subplot(132)\n","    plt.title('loss')\n","    plt.plot(losses)   \n","    plt.show() "],"metadata":{"id":"Ie8GJhZrjDLT","executionInfo":{"status":"ok","timestamp":1651927496917,"user_tz":-420,"elapsed":409,"user":{"displayName":"Win Win Phyo","userId":"12747411063281597081"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def train(env, model, eps_by_episode, optimizer, replay_buffer, episodes = 10000, batch_size=32, gamma = 0.99):\n","    losses = []\n","    all_rewards = []\n","    episode_reward = 0\n","    tot_reward = 0\n","    tr = trange(episodes+1, desc='Agent training', leave=True)\n","\n","    # Get initial state input\n","    state = env.reset()\n","\n","    # Execute episodes iterations\n","    for episode in tr:\n","        tr.set_description(\"Agent training (episode{}) Avg Reward {}\".format(episode+1,tot_reward/(episode+1)))\n","        tr.refresh() \n","\n","        # Get initial epsilon greedy action\n","        epsilon = eps_by_episode(episode)\n","        action = model.act(state, epsilon)\n","        \n","        # Take a step\n","        next_state, reward, done, _ = env.step(action)\n","\n","        # Append experience to replay buffer\n","        replay_buffer.push(state, action, reward, next_state, done)\n","\n","        tot_reward += reward\n","        episode_reward += reward\n","        \n","        state = next_state\n","\n","        # Start a new episode if done signal is received\n","        if done:\n","            state = env.reset()\n","            all_rewards.append(episode_reward)\n","            episode_reward = 0\n","\n","        # Train on a batch if we've got enough experience\n","        if len(replay_buffer) > batch_size:\n","            loss = compute_td_loss(model, batch_size, gamma)\n","            losses.append(loss.item())\n","            \n","    plot(episode, all_rewards, losses)  \n","    return model,all_rewards, losses"],"metadata":{"id":"ymqNCj8ojDNK","executionInfo":{"status":"ok","timestamp":1651927506246,"user_tz":-420,"elapsed":1,"user":{"displayName":"Win Win Phyo","userId":"12747411063281597081"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["model,all_rewards, losses = train(env, model, eps_by_episode, optimizer, replay_buffer, episodes = 10000, batch_size=32, gamma = 0.99)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":445},"id":"-dpbxTJ1jVw_","executionInfo":{"status":"error","timestamp":1651928923695,"user_tz":-420,"elapsed":460,"user":{"displayName":"Win Win Phyo","userId":"12747411063281597081"}},"outputId":"64c25c0c-84de-47f8-adc1-f8d2b52b2447"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stderr","text":["Agent training (episode1) Avg Reward 0.0:   0%|          | 0/10001 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  \n","Agent training (episode1) Avg Reward 0.0:   0%|          | 0/10001 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["q_values.shape torch.Size([32, 9, 728])\n","action.shape torch.Size([32])\n","action_.shape torch.Size([32, 1])\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-df5686130275>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps_by_episode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-12-faa58ffdc613>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(env, model, eps_by_episode, optimizer, replay_buffer, episodes, batch_size, gamma)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Train on a batch if we've got enough experience\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_td_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-31-29fbc5667d85>\u001b[0m in \u001b[0;36mcompute_td_loss\u001b[0;34m(model, batch_size, gamma)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0maction_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"action_.shape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mq_value\u001b[0m          \u001b[0;34m=\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;31m#q_value          = q_values.gather(1, action_.unsqueeze(1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mnext_q_value\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mnext_q_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Index tensor must have the same number of dimensions as input tensor"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"-k6NS1ZJjVzN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"SnNjglrtjV1L"},"execution_count":null,"outputs":[]}]}