{\rtf1\ansi\ansicpg1252\cocoartf2576
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 st122314@9a1d87bf22c3:~/Lab10$ /bin/python3 /home/st122314/Lab10/beam_search.py\
Start preparing training data ...\
Reading lines...\
Read 221282 sentence pairs\
Trimmed to 64271 sentence pairs\
Counting words...\
Counted words: 18008\
\
pairs:\
['there .', 'where ?']\
['you have my word . as a gentleman', 'you re sweet .']\
['hi .', 'looks like things worked out tonight huh ?']\
['you know chastity ?', 'i believe we share an art instructor']\
['have fun tonight ?', 'tons']\
['well no . . .', 'then that s all you had to say .']\
['then that s all you had to say .', 'but']\
['but', 'you always been this selfish ?']\
['do you listen to this crap ?', 'what crap ?']\
['what good stuff ?', 'the real you .']\
keep_words 7823 / 18005 = 0.4345\
Trimmed from 64271 pairs to 53165, 0.8272 of total\
Starting Training!\
Initializing ...\
Training...\
Iteration: 100; Percent complete: 0.8%; Average loss: 5.3070\
Iteration: 200; Percent complete: 1.7%; Average loss: 4.6527\
Iteration: 300; Percent complete: 2.5%; Average loss: 4.3967\
Iteration: 400; Percent complete: 3.3%; Average loss: 4.2879\
Iteration: 500; Percent complete: 4.2%; Average loss: 4.1806\
Iteration: 600; Percent complete: 5.0%; Average loss: 4.0683\
Iteration: 700; Percent complete: 5.8%; Average loss: 4.0168\
Iteration: 800; Percent complete: 6.7%; Average loss: 3.9498\
Iteration: 900; Percent complete: 7.5%; Average loss: 3.8872\
Iteration: 1000; Percent complete: 8.3%; Average loss: 3.8207\
Iteration: 1100; Percent complete: 9.2%; Average loss: 3.7593\
Iteration: 1200; Percent complete: 10.0%; Average loss: 3.7044\
Iteration: 1300; Percent complete: 10.8%; Average loss: 3.6422\
Iteration: 1400; Percent complete: 11.7%; Average loss: 3.5868\
Iteration: 1500; Percent complete: 12.5%; Average loss: 3.5488\
Iteration: 1600; Percent complete: 13.3%; Average loss: 3.4775\
Iteration: 1700; Percent complete: 14.2%; Average loss: 3.4311\
Iteration: 1800; Percent complete: 15.0%; Average loss: 3.3923\
Iteration: 1900; Percent complete: 15.8%; Average loss: 3.3407\
Iteration: 2000; Percent complete: 16.7%; Average loss: 3.3037\
content/cb_model/Chat/2-4_512\
Iteration: 2100; Percent complete: 17.5%; Average loss: 3.2544\
Iteration: 2200; Percent complete: 18.3%; Average loss: 3.2259\
Iteration: 2300; Percent complete: 19.2%; Average loss: 3.1731\
Iteration: 2400; Percent complete: 20.0%; Average loss: 3.1425\
Iteration: 2500; Percent complete: 20.8%; Average loss: 3.0807\
Iteration: 2600; Percent complete: 21.7%; Average loss: 3.0427\
Iteration: 2700; Percent complete: 22.5%; Average loss: 3.0337\
Iteration: 2800; Percent complete: 23.3%; Average loss: 2.9592\
Iteration: 2900; Percent complete: 24.2%; Average loss: 2.9324\
Iteration: 3000; Percent complete: 25.0%; Average loss: 2.8996\
Iteration: 3100; Percent complete: 25.8%; Average loss: 2.8528\
Iteration: 3200; Percent complete: 26.7%; Average loss: 2.8014\
Iteration: 3300; Percent complete: 27.5%; Average loss: 2.7761\
Iteration: 3400; Percent complete: 28.3%; Average loss: 2.7320\
Iteration: 3500; Percent complete: 29.2%; Average loss: 2.7024\
Iteration: 3600; Percent complete: 30.0%; Average loss: 2.6705\
Iteration: 3700; Percent complete: 30.8%; Average loss: 2.6362\
Iteration: 3800; Percent complete: 31.7%; Average loss: 2.5955\
Iteration: 3900; Percent complete: 32.5%; Average loss: 2.5634\
Iteration: 4000; Percent complete: 33.3%; Average loss: 2.5134\
content/cb_model/Chat/2-4_512\
Iteration: 4100; Percent complete: 34.2%; Average loss: 2.4784\
Iteration: 4200; Percent complete: 35.0%; Average loss: 2.4509\
Iteration: 4300; Percent complete: 35.8%; Average loss: 2.4266\
Iteration: 4400; Percent complete: 36.7%; Average loss: 2.3841\
Iteration: 4500; Percent complete: 37.5%; Average loss: 2.3469\
Iteration: 4600; Percent complete: 38.3%; Average loss: 2.3299\
Iteration: 4700; Percent complete: 39.2%; Average loss: 2.2926\
Iteration: 4800; Percent complete: 40.0%; Average loss: 2.2482\
Iteration: 4900; Percent complete: 40.8%; Average loss: 2.2217\
Iteration: 5000; Percent complete: 41.7%; Average loss: 2.1911\
Iteration: 5100; Percent complete: 42.5%; Average loss: 2.1572\
Iteration: 5200; Percent complete: 43.3%; Average loss: 2.1202\
Iteration: 5300; Percent complete: 44.2%; Average loss: 2.0856\
Iteration: 5400; Percent complete: 45.0%; Average loss: 2.0642\
Iteration: 5500; Percent complete: 45.8%; Average loss: 2.0391\
Iteration: 5600; Percent complete: 46.7%; Average loss: 1.9981\
Iteration: 5700; Percent complete: 47.5%; Average loss: 1.9735\
Iteration: 5800; Percent complete: 48.3%; Average loss: 1.9402\
Iteration: 5900; Percent complete: 49.2%; Average loss: 1.9202\
Iteration: 6000; Percent complete: 50.0%; Average loss: 1.8856\
content/cb_model/Chat/2-4_512\
Iteration: 6100; Percent complete: 50.8%; Average loss: 1.8527\
Iteration: 6200; Percent complete: 51.7%; Average loss: 1.8236\
Iteration: 6300; Percent complete: 52.5%; Average loss: 1.7926\
Iteration: 6400; Percent complete: 53.3%; Average loss: 1.7556\
Iteration: 6500; Percent complete: 54.2%; Average loss: 1.7305\
Iteration: 6600; Percent complete: 55.0%; Average loss: 1.6976\
Iteration: 6700; Percent complete: 55.8%; Average loss: 1.6857\
Iteration: 6800; Percent complete: 56.7%; Average loss: 1.6450\
Iteration: 6900; Percent complete: 57.5%; Average loss: 1.6235\
Iteration: 7000; Percent complete: 58.3%; Average loss: 1.6138\
Iteration: 7100; Percent complete: 59.2%; Average loss: 1.5743\
Iteration: 7200; Percent complete: 60.0%; Average loss: 1.5397\
Iteration: 7300; Percent complete: 60.8%; Average loss: 1.5149\
Iteration: 7400; Percent complete: 61.7%; Average loss: 1.5023\
Iteration: 7500; Percent complete: 62.5%; Average loss: 1.4609\
Iteration: 7600; Percent complete: 63.3%; Average loss: 1.4424\
Iteration: 7700; Percent complete: 64.2%; Average loss: 1.4171\
Iteration: 7800; Percent complete: 65.0%; Average loss: 1.3851\
Iteration: 7900; Percent complete: 65.8%; Average loss: 1.3616\
Iteration: 8000; Percent complete: 66.7%; Average loss: 1.3330\
content/cb_model/Chat/2-4_512\
Iteration: 8100; Percent complete: 67.5%; Average loss: 1.3193\
Iteration: 8200; Percent complete: 68.3%; Average loss: 1.2993\
Iteration: 8300; Percent complete: 69.2%; Average loss: 1.2645\
Iteration: 8400; Percent complete: 70.0%; Average loss: 1.2376\
Iteration: 8500; Percent complete: 70.8%; Average loss: 1.2215\
Iteration: 8600; Percent complete: 71.7%; Average loss: 1.2134\
Iteration: 8700; Percent complete: 72.5%; Average loss: 1.1757\
Iteration: 8800; Percent complete: 73.3%; Average loss: 1.1644\
Iteration: 8900; Percent complete: 74.2%; Average loss: 1.1472\
Iteration: 9000; Percent complete: 75.0%; Average loss: 1.1251\
Iteration: 9100; Percent complete: 75.8%; Average loss: 1.1132\
Iteration: 9200; Percent complete: 76.7%; Average loss: 1.0802\
Iteration: 9300; Percent complete: 77.5%; Average loss: 1.0645\
Iteration: 9400; Percent complete: 78.3%; Average loss: 1.0478\
Iteration: 9500; Percent complete: 79.2%; Average loss: 1.0277\
Iteration: 9600; Percent complete: 80.0%; Average loss: 1.0020\
Iteration: 9700; Percent complete: 80.8%; Average loss: 0.9879\
Iteration: 9800; Percent complete: 81.7%; Average loss: 0.9736\
Iteration: 9900; Percent complete: 82.5%; Average loss: 0.9519\
Iteration: 10000; Percent complete: 83.3%; Average loss: 0.9386\
content/cb_model/Chat/2-4_512\
Iteration: 10100; Percent complete: 84.2%; Average loss: 0.9227\
Iteration: 10200; Percent complete: 85.0%; Average loss: 0.9086\
Iteration: 10300; Percent complete: 85.8%; Average loss: 0.8902\
Iteration: 10400; Percent complete: 86.7%; Average loss: 0.8632\
Iteration: 10500; Percent complete: 87.5%; Average loss: 0.8526\
Iteration: 10600; Percent complete: 88.3%; Average loss: 0.8487\
Iteration: 10700; Percent complete: 89.2%; Average loss: 0.8316\
Iteration: 10800; Percent complete: 90.0%; Average loss: 0.8114\
Iteration: 10900; Percent complete: 90.8%; Average loss: 0.8068\
Iteration: 11000; Percent complete: 91.7%; Average loss: 0.7876\
Iteration: 11100; Percent complete: 92.5%; Average loss: 0.7707\
Iteration: 11200; Percent complete: 93.3%; Average loss: 0.7672\
Iteration: 11300; Percent complete: 94.2%; Average loss: 0.7534\
Iteration: 11400; Percent complete: 95.0%; Average loss: 0.7365\
Iteration: 11500; Percent complete: 95.8%; Average loss: 0.7250\
Iteration: 11600; Percent complete: 96.7%; Average loss: 0.7108\
Iteration: 11700; Percent complete: 97.5%; Average loss: 0.6970\
Iteration: 11800; Percent complete: 98.3%; Average loss: 0.6858\
Iteration: 11900; Percent complete: 99.2%; Average loss: 0.6898\
Iteration: 12000; Percent complete: 100.0%; Average loss: 0.6710\
content/cb_model/Chat/2-4_512\
0 0.12500000000000003 0.04225771273642583\
1000 0.14285224359219448 0.060097084653048874\
Traceback (most recent call last):\
  File "/home/st122314/Lab10/beam_search.py", line 211, in <module>\
    output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\
  File "/home/st122314/Lab10/test.py", line 19, in evaluate\
    tokens, scores = searcher(input_batch, lengths, max_length)\
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1102, in _call_impl\
    return forward_call(*input, **kwargs)\
  File "/home/st122314/Lab10/beam_search.py", line 127, in forward\
    sentences = beam_decode(self.decoder, decoder_hidden, encoder_outputs, self.voc, self.beam_size, max_length)\
  File "/home/st122314/Lab10/beam_search.py", line 95, in beam_decode\
    term, top = sentence.addTopk(topi, topv, decoder_hidden, beam_size, voc)\
  File "/home/st122314/Lab10/beam_search.py", line 55, in addTopk\
    self.avgScore())) \
  File "/home/st122314/Lab10/beam_search.py", line 45, in avgScore\
    raise ValueError("Calculate average score of sentence, but got no word")\
ValueError: Calculate average score of sentence, but got no word\
\
\
\
\
\
content/cb_model/Chat/2-4_512\
0 0.08786571270524224 0.03403024420100487\
1000 0.13560245731142448 0.054173508766110434\
2000 0.13202144929246235 0.0547724869705568\
3000 0.13083288721324685 0.054209945046619204\
4000 0.133090986999077 0.05569587503441644\
5000 0.13191480116890775 0.055359817765740615\
6000 0.1312868774694944 0.05512274845234756\
7000 0.14314621766285207 0.06683194587443625\
8000 0.14180658707962898 0.06525827987485991\
Total Bleu Score for 1 grams on testing pairs:  0.14142280489931194\
Total Bleu Score for 2 grams on testing pairs:  0.06491881943777443\
> hello\
Bot: do you want to see doc ?\
> sure\
Bot: do you want a perfect for it ?\
> of course\
Bot: if you trusted me\
> yes\
Bot: why ?\
> because you are a bot\
Error: Encountered unknown word.\
> }